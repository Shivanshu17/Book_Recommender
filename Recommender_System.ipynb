{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/useriteminteractionspoetry/interactions_poetry.csv\n/kaggle/input/goodbooks-books-poetry/goodreads_books_poetry.json\n/kaggle/input/goodreads-interactions-poetry/goodreads_interactions_poetry.json\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install similaripy\n!pip install scikit-surprise","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting similaripy\n  Downloading similaripy-0.1.1.tar.gz (334 kB)\n\u001b[K     |████████████████████████████████| 334 kB 194 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from similaripy) (1.4.1)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from similaripy) (1.18.1)\nRequirement already satisfied: tqdm>=4.19.6 in /opt/conda/lib/python3.7/site-packages (from similaripy) (4.45.0)\nBuilding wheels for collected packages: similaripy\n  Building wheel for similaripy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for similaripy: filename=similaripy-0.1.1-cp37-cp37m-linux_x86_64.whl size=1515922 sha256=5ac29d7946c98221a3ae4e56dc154962151d34feb8705253e071f6b72e65e71a\n  Stored in directory: /root/.cache/pip/wheels/52/77/1c/8da70f5b02be559b78118a9867c2fd864ed1d221d3d440ee95\nSuccessfully built similaripy\nInstalling collected packages: similaripy\nSuccessfully installed similaripy-0.1.1\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: scikit-surprise in /opt/conda/lib/python3.7/site-packages (1.1.0)\nRequirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (1.4.1)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (1.14.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (0.14.1)\nRequirement already satisfied: numpy>=1.11.2 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (1.18.1)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nfrom pandas.io.json import json_normalize\nfrom sys import getsizeof\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom collections import defaultdict\n\nfrom scipy.sparse import csr_matrix, csc_matrix, coo_matrix\nimport scipy.sparse as sp\nfrom scipy import *\n\nimport similaripy as sim\nfrom similaripy.normalization import normalize, bm25, bm25plus, tfidf\n\nfrom implicit.nearest_neighbours import ItemItemRecommender, bm25_weight, tfidf_weight\nimport implicit\n\nfrom surprise import SVD\nfrom surprise import SVDpp\nfrom surprise.model_selection import cross_validate\nfrom surprise import BaselineOnly\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise.model_selection import train_test_split\n\n# For displaying images\nfrom IPython.display import Image \nfrom matplotlib.pyplot import figure, imshow, axis\nfrom matplotlib.image import imread\nimport requests\n","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Data\n\nThe following two cells are temporary, and will be removed once we have the interactions and book data in the required formats"},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions_df = pd.read_csv('/kaggle/input/useriteminteractionspoetry/interactions_poetry.csv')\ndisplay(interactions_df.head(5))\ndisplay(interactions_df.describe())\n#display(interactions_df.info)","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"   Unnamed: 0                           user_id   book_id  \\\n0           0  8842281e1d1347389f2ab93d60773d4d      1384   \n1           1  8842281e1d1347389f2ab93d60773d4d      1376   \n2           2  8842281e1d1347389f2ab93d60773d4d     30119   \n3           3  72fb0d0087d28c832f15776b0d936598  24769928   \n4           4  72fb0d0087d28c832f15776b0d936598     30119   \n\n                          review_id  is_read  rating review_text_incomplete  \\\n0  1bad0122cebb4aa9213f9fe1aa281f66     True       4                    NaN   \n1  eb6e502d0c04d57b43a5a02c21b64ab4     True       4                    NaN   \n2  787564bef16cb1f43e0f641ab59d25b7     True       5                    NaN   \n3  8c80ee74743d4b3b123dd1a2e0c0bcac    False       0                    NaN   \n4  2a83589fb597309934ec9b1db5876aaf     True       3                    NaN   \n\n                       date_added                    date_updated  \\\n0  Wed May 09 09:33:44 -0700 2007  Wed May 09 09:33:44 -0700 2007   \n1  Wed May 09 09:33:18 -0700 2007  Wed May 09 09:33:18 -0700 2007   \n2  Sat Jan 13 13:44:20 -0800 2007  Wed Mar 22 11:45:08 -0700 2017   \n3  Wed Apr 27 11:05:51 -0700 2016  Wed Apr 27 11:05:52 -0700 2016   \n4  Mon Jun 04 18:58:08 -0700 2012  Mon Jun 04 18:58:13 -0700 2012   \n\n                          read_at started_at  \n0                             NaN        NaN  \n1                             NaN        NaN  \n2  Tue Mar 01 00:00:00 -0800 1983        NaN  \n3                             NaN        NaN  \n4                             NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>is_read</th>\n      <th>rating</th>\n      <th>review_text_incomplete</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1384</td>\n      <td>1bad0122cebb4aa9213f9fe1aa281f66</td>\n      <td>True</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>Wed May 09 09:33:44 -0700 2007</td>\n      <td>Wed May 09 09:33:44 -0700 2007</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1376</td>\n      <td>eb6e502d0c04d57b43a5a02c21b64ab4</td>\n      <td>True</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>Wed May 09 09:33:18 -0700 2007</td>\n      <td>Wed May 09 09:33:18 -0700 2007</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>30119</td>\n      <td>787564bef16cb1f43e0f641ab59d25b7</td>\n      <td>True</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>Sat Jan 13 13:44:20 -0800 2007</td>\n      <td>Wed Mar 22 11:45:08 -0700 2017</td>\n      <td>Tue Mar 01 00:00:00 -0800 1983</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>72fb0d0087d28c832f15776b0d936598</td>\n      <td>24769928</td>\n      <td>8c80ee74743d4b3b123dd1a2e0c0bcac</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Wed Apr 27 11:05:51 -0700 2016</td>\n      <td>Wed Apr 27 11:05:52 -0700 2016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>72fb0d0087d28c832f15776b0d936598</td>\n      <td>30119</td>\n      <td>2a83589fb597309934ec9b1db5876aaf</td>\n      <td>True</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>Mon Jun 04 18:58:08 -0700 2012</td>\n      <td>Mon Jun 04 18:58:13 -0700 2012</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         Unnamed: 0       book_id        rating\ncount  2.734350e+06  2.734350e+06  2.734350e+06\nmean   1.367174e+06  6.808744e+06  1.824787e+00\nstd    7.893390e+05  9.698381e+06  2.123223e+00\nmin    0.000000e+00  2.340000e+02  0.000000e+00\n25%    6.835872e+05  4.204000e+04  0.000000e+00\n50%    1.367174e+06  5.922210e+05  0.000000e+00\n75%    2.050762e+06  1.219330e+07  4.000000e+00\nmax    2.734349e+06  3.648548e+07  5.000000e+00","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.734350e+06</td>\n      <td>2.734350e+06</td>\n      <td>2.734350e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.367174e+06</td>\n      <td>6.808744e+06</td>\n      <td>1.824787e+00</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.893390e+05</td>\n      <td>9.698381e+06</td>\n      <td>2.123223e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>2.340000e+02</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6.835872e+05</td>\n      <td>4.204000e+04</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.367174e+06</td>\n      <td>5.922210e+05</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.050762e+06</td>\n      <td>1.219330e+07</td>\n      <td>4.000000e+00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.734349e+06</td>\n      <td>3.648548e+07</td>\n      <td>5.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions_df.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this cell, I'll clean all the data, and make sure there aren't any anomalies\n\n\n# interactions_df.drop(['Unnamed: 0'], axis = 1, inplace = True)\ninteractions_df = interactions_df.drop(columns = ['review_id','is_read','review_text_incomplete','date_added','date_updated','read_at','started_at'])\n\n# I still have to remove all the reviews with 0 ratings. Becuase, they haven't actually read the book\ninteractions_df = interactions_df[interactions_df.rating != 0]\n\ninteractions_df['user_idInt']= interactions_df['user_id'].str.replace('\\D+','').astype(float)\ninteractions_df = interactions_df.drop(columns = ['user_id'])\ndisplay(interactions_df.head())","execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"   Unnamed: 0  book_id  rating    user_idInt\n0           0     1384       4  8.842281e+23\n1           1     1376       4  8.842281e+23\n2           2    30119       5  8.842281e+23\n4           4    30119       3  7.200087e+23\n5           5   240007       4  2.923738e+15","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>book_id</th>\n      <th>rating</th>\n      <th>user_idInt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1384</td>\n      <td>4</td>\n      <td>8.842281e+23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1376</td>\n      <td>4</td>\n      <td>8.842281e+23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>30119</td>\n      <td>5</td>\n      <td>8.842281e+23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>30119</td>\n      <td>3</td>\n      <td>7.200087e+23</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>240007</td>\n      <td>4</td>\n      <td>2.923738e+15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions_df.drop(['Unnamed: 0'], axis = 1, inplace = True)\ninteractions_df.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"   book_id  rating    user_idInt\n0     1384       4  8.842281e+23\n1     1376       4  8.842281e+23\n2    30119       5  8.842281e+23\n4    30119       3  7.200087e+23\n5   240007       4  2.923738e+15","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>rating</th>\n      <th>user_idInt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1384</td>\n      <td>4</td>\n      <td>8.842281e+23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1376</td>\n      <td>4</td>\n      <td>8.842281e+23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30119</td>\n      <td>5</td>\n      <td>8.842281e+23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30119</td>\n      <td>3</td>\n      <td>7.200087e+23</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>240007</td>\n      <td>4</td>\n      <td>2.923738e+15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions_df.describe()","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"            book_id        rating    user_idInt\ncount  1.229059e+06  1.229059e+06  1.229059e+06\nmean   4.762292e+06  4.059696e+00  3.621602e+25\nstd    8.235111e+06  9.779034e-01  6.299588e+27\nmin    2.340000e+02  1.000000e+00  1.166294e+06\n25%    3.011800e+04  3.000000e+00  5.787726e+17\n50%    2.661260e+05  4.000000e+00  5.239819e+19\n75%    6.349059e+06  5.000000e+00  4.155079e+21\nmax    3.648548e+07  5.000000e+00  3.874271e+30","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>rating</th>\n      <th>user_idInt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1.229059e+06</td>\n      <td>1.229059e+06</td>\n      <td>1.229059e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>4.762292e+06</td>\n      <td>4.059696e+00</td>\n      <td>3.621602e+25</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>8.235111e+06</td>\n      <td>9.779034e-01</td>\n      <td>6.299588e+27</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2.340000e+02</td>\n      <td>1.000000e+00</td>\n      <td>1.166294e+06</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>3.011800e+04</td>\n      <td>3.000000e+00</td>\n      <td>5.787726e+17</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.661260e+05</td>\n      <td>4.000000e+00</td>\n      <td>5.239819e+19</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.349059e+06</td>\n      <td>5.000000e+00</td>\n      <td>4.155079e+21</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.648548e+07</td>\n      <td>5.000000e+00</td>\n      <td>3.874271e+30</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(interactions_df['book_id'].unique())","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"36182"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"books_data = []\nwith open(\"/kaggle/input/goodbooks-books-poetry/goodreads_books_poetry.json\", 'r') as f:\n    for line in f:\n        books_data.append(json.loads(line))\n\nbook_df = pd.DataFrame(books_data)\n\ndisplay(book_df.head())","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"         isbn text_reviews_count series country_code language_code  \\\n0                              1     []           US           eng   \n1  0811223981                  2     []           US                 \n2  0374428115                  7     []           US                 \n3  0156182890                 12     []           US                 \n4  1942004192                  4     []           US           eng   \n\n                                     popular_shelves asin is_ebook  \\\n0  [{'count': '8', 'name': 'to-read'}, {'count': ...         false   \n1  [{'count': '100', 'name': 'to-read'}, {'count'...         false   \n2  [{'count': '32', 'name': 'to-read'}, {'count':...         false   \n3  [{'count': '554', 'name': 'to-read'}, {'count'...         false   \n4  [{'count': '228', 'name': 'to-read'}, {'count'...         false   \n\n  average_rating kindle_asin  ... publication_month edition_information  \\\n0           3.83              ...                11                       \n1           3.83  B00U2WY9U8  ...                 4                       \n2           4.38              ...                 7                       \n3           3.71  B00IWTRB1W  ...                 3                       \n4           5.00              ...                12               First   \n\n  publication_year                                                url  \\\n0             1887  https://www.goodreads.com/book/show/16037549-v...   \n1             2015  https://www.goodreads.com/book/show/22466716-f...   \n2             2008  https://www.goodreads.com/book/show/926662.Gro...   \n3             1964  https://www.goodreads.com/book/show/926667.The...   \n4             2015  https://www.goodreads.com/book/show/29065952-l...   \n\n                                           image_url   book_id ratings_count  \\\n0  https://images.gr-assets.com/books/1348176637m...  16037549             3   \n1  https://images.gr-assets.com/books/1404958407m...  22466716            37   \n2  https://s.gr-assets.com/assets/nophoto/book/11...    926662            45   \n3  https://images.gr-assets.com/books/1382939971m...    926667           115   \n4  https://images.gr-assets.com/books/1455198396m...  29065952             9   \n\n    work_id                                    title  \\\n0   5212748    Vision of Sir Launfal and Other Poems   \n1  41905435                 Fairy Tales: Dramolettes   \n2    911665  Growltiger's Last Stand and Other Poems   \n3    995066                       The Cocktail Party   \n4  49294781          Louder Than Everything You Love   \n\n                      title_without_series  \n0    Vision of Sir Launfal and Other Poems  \n1                 Fairy Tales: Dramolettes  \n2  Growltiger's Last Stand and Other Poems  \n3                       The Cocktail Party  \n4          Louder Than Everything You Love  \n\n[5 rows x 29 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isbn</th>\n      <th>text_reviews_count</th>\n      <th>series</th>\n      <th>country_code</th>\n      <th>language_code</th>\n      <th>popular_shelves</th>\n      <th>asin</th>\n      <th>is_ebook</th>\n      <th>average_rating</th>\n      <th>kindle_asin</th>\n      <th>...</th>\n      <th>publication_month</th>\n      <th>edition_information</th>\n      <th>publication_year</th>\n      <th>url</th>\n      <th>image_url</th>\n      <th>book_id</th>\n      <th>ratings_count</th>\n      <th>work_id</th>\n      <th>title</th>\n      <th>title_without_series</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td>1</td>\n      <td>[]</td>\n      <td>US</td>\n      <td>eng</td>\n      <td>[{'count': '8', 'name': 'to-read'}, {'count': ...</td>\n      <td></td>\n      <td>false</td>\n      <td>3.83</td>\n      <td></td>\n      <td>...</td>\n      <td>11</td>\n      <td></td>\n      <td>1887</td>\n      <td>https://www.goodreads.com/book/show/16037549-v...</td>\n      <td>https://images.gr-assets.com/books/1348176637m...</td>\n      <td>16037549</td>\n      <td>3</td>\n      <td>5212748</td>\n      <td>Vision of Sir Launfal and Other Poems</td>\n      <td>Vision of Sir Launfal and Other Poems</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0811223981</td>\n      <td>2</td>\n      <td>[]</td>\n      <td>US</td>\n      <td></td>\n      <td>[{'count': '100', 'name': 'to-read'}, {'count'...</td>\n      <td></td>\n      <td>false</td>\n      <td>3.83</td>\n      <td>B00U2WY9U8</td>\n      <td>...</td>\n      <td>4</td>\n      <td></td>\n      <td>2015</td>\n      <td>https://www.goodreads.com/book/show/22466716-f...</td>\n      <td>https://images.gr-assets.com/books/1404958407m...</td>\n      <td>22466716</td>\n      <td>37</td>\n      <td>41905435</td>\n      <td>Fairy Tales: Dramolettes</td>\n      <td>Fairy Tales: Dramolettes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0374428115</td>\n      <td>7</td>\n      <td>[]</td>\n      <td>US</td>\n      <td></td>\n      <td>[{'count': '32', 'name': 'to-read'}, {'count':...</td>\n      <td></td>\n      <td>false</td>\n      <td>4.38</td>\n      <td></td>\n      <td>...</td>\n      <td>7</td>\n      <td></td>\n      <td>2008</td>\n      <td>https://www.goodreads.com/book/show/926662.Gro...</td>\n      <td>https://s.gr-assets.com/assets/nophoto/book/11...</td>\n      <td>926662</td>\n      <td>45</td>\n      <td>911665</td>\n      <td>Growltiger's Last Stand and Other Poems</td>\n      <td>Growltiger's Last Stand and Other Poems</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0156182890</td>\n      <td>12</td>\n      <td>[]</td>\n      <td>US</td>\n      <td></td>\n      <td>[{'count': '554', 'name': 'to-read'}, {'count'...</td>\n      <td></td>\n      <td>false</td>\n      <td>3.71</td>\n      <td>B00IWTRB1W</td>\n      <td>...</td>\n      <td>3</td>\n      <td></td>\n      <td>1964</td>\n      <td>https://www.goodreads.com/book/show/926667.The...</td>\n      <td>https://images.gr-assets.com/books/1382939971m...</td>\n      <td>926667</td>\n      <td>115</td>\n      <td>995066</td>\n      <td>The Cocktail Party</td>\n      <td>The Cocktail Party</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1942004192</td>\n      <td>4</td>\n      <td>[]</td>\n      <td>US</td>\n      <td>eng</td>\n      <td>[{'count': '228', 'name': 'to-read'}, {'count'...</td>\n      <td></td>\n      <td>false</td>\n      <td>5.00</td>\n      <td></td>\n      <td>...</td>\n      <td>12</td>\n      <td>First</td>\n      <td>2015</td>\n      <td>https://www.goodreads.com/book/show/29065952-l...</td>\n      <td>https://images.gr-assets.com/books/1455198396m...</td>\n      <td>29065952</td>\n      <td>9</td>\n      <td>49294781</td>\n      <td>Louder Than Everything You Love</td>\n      <td>Louder Than Everything You Love</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 29 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"book_df['book_id'] = book_df['book_id'].astype(int)\nbook_df['image_url'] = book_df['image_url'].astype(str)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(book_df.describe())","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"            book_id\ncount  3.651400e+04\nmean   1.063452e+07\nstd    1.035345e+07\nmin    2.340000e+02\n25%    1.185514e+06\n50%    7.223308e+06\n75%    1.821872e+07\nmax    3.648548e+07","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3.651400e+04</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.063452e+07</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.035345e+07</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2.340000e+02</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.185514e+06</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.223308e+06</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.821872e+07</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>3.648548e+07</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"exists = 37100000 in book_df.book_id\nprint(exists)","execution_count":12,"outputs":[{"output_type":"stream","text":"False\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Supporting Functions\nHere I'll try to convert the entire data in form of scipy.sparse matrices, and download that data using scipy.sparse.save_npz(). This way, perhaps, our memory usage would be a bit lesser, and we could fit more of the data in the 16 Gigs we have at our disposal."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef showImagesHorizontally(list_of_files):\n    '''\n    This function shows all the books in a horizontal row within the ipython notebook\n    '''\n    fig = figure()\n    number_of_files = len(list_of_files)\n    for i in range(number_of_files):\n        a=fig.add_subplot(1,number_of_files,i+1)\n        image = imread(list_of_files[i])\n        imshow(image)\n        axis('off')\n\n        \ndef print_book_images(book_df, recommendations):\n    '''\n    This function will display the images along with Titles and author names of all the books recommended \n    Args:\n        recommendations (iterable) -> Containing the book_ids of the recommendations made by the recommender system\n        book_df (DataFrame) -> DataFrame containing basic info of all the books part of our product ('book_id','url','image_url','authors', 'title')\n        \n        \n    '''\n    #import requests\n    import IPython.display as Disp\n    url = 'https://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg'\n    Disp.Image(requests.get(url).content)\n    temp_df = book_df.loc[book_df['book_id'].isin(recommendations)]\n    # display(book_df.head())\n    display(temp_df.head())\n    width_pixels = 200\n    height_pixels = 300\n    book_urls = []\n    for id in recommendations:\n        book = book_df.loc[book_df['book_id'] == id]\n        image_url = book['image_url'].astype(str)\n        print(image_url)\n        book_urls.append(image_url)\n        title = book['title']\n        authors = book['authors']\n        # Image(requests.get(image_url).content,width=width_pixels, height=height_pixels)\n        Image(url = image_url,width=width_pixels, height=height_pixels)\n        print('Title : ', title)\n        print('Authors : ', authors)\n        #width_pixels = width_pixel - 10\n        #height_pixels = height_pixels - 8\n    \n    # showImagesHorizontally(book_urls)\n    \n    \n    \ndef print_book_ratings(data):\n    '''\n    This function will print the book ratings of the top-k book recommendations passed along to it\n    \n    '''\n    ","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Functions for Downloading the sparce matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_sparse_matrix(data, user_dict, item_dict):\n    '''\n    This function creates a csr sparse matrix from the given data. Data should be a Dataframe (preferably derived from a .csv file)\n    \n    Args:\n        data (DataFrame) -> Containing book_id, user_idInt, and rating for each instance\n        user_dict(Pandas series) -> Contains the unique set of user ids present within the data\n        item_dict (Pandas Series) -> Contains the unique set of items present in the data\n    Returns:\n        csr_item_matrix (sparse csr matrix) -> csr item matrix created from the data\n        user_dict, item_dict -> Both updated\n    '''\n    # Here I'll create 'user_dict' & 'item_dict' such that: the indexes of the array would represent the \"key\" for the dicts.\n    temp_user_dict = pd.concat([user_dict, pd.unique(data['user_idInt'])], ignore_index = True)\n    temp_item_dict = pd.append(pd.unique(data['book_id']), ignore_index = True)\n    user_dict = pd.unique(temp_user_dict)\n    item_dict = pd.unique(temp_item_dict)\n\n    # Then, implement the csr_mechanism and save the result in self.csr_item_matrix\n    rows = []\n    cols = []\n    data = []\n    for i in range(len(data)):\n        rows.append(user_dict.index(data.loc[i, 'user_idInt'])) # Here we creating the row values for csr matrix using the index values of the user_dict we created by mapping unique values in a dataframe\n        cols.append(item_dict.index(data.loc[i, 'book_id'])) # Maybe, the index part should be in square brackets as well... Will see\n        data.append(data.loc[i, 'rating'])\n    rows = np.array(rows)\n    cols = np.array(cols)\n    data = np.array(data)\n    csr_item_matrix = csr_matrix(data, (row, col))\n    return csr_item_matrix.tocsr(), user_dict, item_dict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here I'll create the sparse matrix using create_sparse_matrix function and then download it. Following which I'll upload it directly, and see if I can save further memory space this way\ndef download_sparse_matrix(filename, csr_matrix):\n    '''\n    This function will download the sparse matrix. I will be calling this function multiple times to convert and store all the interactions data in this very format.\n    \n    Actually, I might not have to make a seperate npz file for each genre data, instead, I can create a single npz file with multiple (compressed) chunks of data from \n    all the genres. The good thing about npz format is that it allows for multiple files to be stored such that only the ones being used are loaded onto the memory.\n    Checkout -https://stackoverflow.com/questions/54238670/what-is-the-advantage-of-saving-npz-files-instead-of-npy-in-python-regard\n    \n    \n    '''\n    sp.save_npz(filename, matrix)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Implementation code for the supporting functions\n\nHere, we'll be calling all the supporting functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I will clean the interactions data (removing unnecessary columns: maybe this can be done in a function, so that different data can be produced for implicit and explicit models) & call the two functions above. \n\n# Create_sparse_Matrix will probably have to be called for each genre or chunk we want, while (hopefully) download sparse can be called once.\n#  I will also have to look into how to add files to .npz format. Cos, maybe I can't load all the json data and convert it into npy files in one go.\n\n\n# Create user_dict and item_dict (to maintain uniqueness across varioud datasets)\nuser_dict = pd.DataFrame()\nitem_dict = pd.DataFrame()\nbook_df = pd.DataFrame(columns = ['book_id','url','image_url','authors', 'title'])\n# Perform the following code for each of the genre, and make sure user_dict and item_dict are maintained along all of them\n\n# Convert data and add it in form of lists\ninteractions_data = []\nwith open(\"/kaggle/input/goodreads-interactions-poetry/goodreads_interactions_poetry.json\", 'r') as f:\n    for line in f:\n        interaction_data.append(json.loads(line))\n\nbooks_data = []\nwith open(\"/kaggle/input/goodbooks-books-poetry/goodreads_books_poetry.json\", 'r') as f:\n    for line in f:\n        books_data.append(json.loads(line))\n\n# Convert it into a dataframe and store it according to genre name\ninteraction_df_poetry = pd.DataFrame(interaction_data)\nbook_df_poetry = pd.DataFrame(books_data)\n\n# Covert it into a csv file\n#interaction_df_poetry.to_csv('interactions_poetry.csv')\n\n# Drop the unnecessary columns and change the data type of the user_id column\ninteraction_df_poetry = interaction_df_poetry.drop(columns = ['review_id','is_read','review_text_incomplete','date_added','date_updated','read_at','started_at'])\nbook_df_poetry = book_df_poetry.drop(columns = ['work_id','isbn','series','asin','kindle_asin','description','link','isbn13','edition_information','title_without_series','country_code','text_reviews_count','language_code','publication_day','publication_month','is_ebook','format','publication_year','ratings_count', 'popular_shelves','publisher','similar_books'])\ninteraction_df_poetry['user_idInt']= interaction_df_poetry['user_id'].str.replace('\\D+','').astype(float)\n\n# Create and download the csr matrices\npoetry_csr_matrix, user_dict, item_dict = create_sparse_matrix(interaction_df_poetry, user_dict, item_dict)\ndownload_sparse_matrix('interactions_poetry', poetry_csr_matrix)\n\n# Create a unified book_df, to store: book_id, url, image_url, authors, and title of each book. Make sure that the book_df variable doesn't go away on each iteration of coding\nbook_df = pd.concat([book_df, book_df_poetry], ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This part is intended to convert the json files of various genres (the original data form), into .csv files.\n# I have to put it in a function, so that it can be used effectively.\n'''interaction_data = []\nwith open(\"/kaggle/input/goodreads-interactions-poetry/goodreads_interactions_poetry.json\", 'r') as f:\n    for line in f:\n        interaction_data.append(json.loads(line))\n        \n#interaction_data1 = interaction_data[:1000000]\ninteraction_df_poetry = pd.DataFrame(interaction_data)\ninteraction_df_poetry.head(10)\ndisplay(interaction_df_poetry.describe())\ninteraction_df_poetry.to_csv('interactions_poetry.csv')\nprint(getsizeof(interaction_data1))\nprint(\"Dataframe size is\", getsizeof(interaction_df1))\noriginal_df = pd.DataFrame(columns = ['user_id', 'book_id', 'review_id', 'is_read', 'rating', 'review_text_incomplete', 'date_added', 'date_updated', 'read_at', 'started_at'])\ntemp_df = pd.DataFrame(columns = ['user_id', 'book_id', 'review_id', 'is_read', 'rating', 'review_text_incomplete', 'date_added', 'date_updated', 'read_at', 'started_at'])\nwith open(\"/kaggle/input/goodreads-interactions-poetry/goodreads_interactions_poetry.json\", 'r') as f:\n    for line in f:\n        data = json.loads(line)\n        original_df = pd.concat([original_df, pd.json_normalize(data)])\n        #original_df.concat(json_normalize(data, '0'))\ndisplay(interaction_df1.head())\ndisplay(interaction_df1.info())\n'''\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 1: Neighborhood Modelling\nI will try to do this part in the following two steps:\n1. I will create the sparse matrix of the data and use them on distance metric code from scratch\n2. Use the same sparse matrix to then evaluate the results using the implementation of similaripy"},{"metadata":{},"cell_type":"markdown","source":"### 1. Using Sparse Matrices and simple Cosine Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"class knn_original():\n    def __init__(self, k = 20, data = None, distance_metric = 'dot_product', is_already_sparse = False, csr_item_matrix = None, alpha = 0.6):\n        '''\n        This class produces recommendations based on item-item similarity measures.\n        The distance metrics used in this class are written from scratch, and don't have the speed benefits of the cython used by the library implementation following this.\n        \n        Args:\n            data (DataFrame) -> Containing user_item interactions and their ratings.\n            distance_metric (object) -> Defines the distance metric to be used\n            k (integer) -> Represents the number of similar items that we have to produce\n            csr_item_matrix (sparse matrix) -> csr matrix with items as rows and users as columns (optional)\n            is_already_sparse (boolean) -> Defines if the data is already sparse.\n            \n        Returns:\n            similarity_matrix (sparse_matrix) -> Contains the top-k similar items for each item of the data entered.\n            \n        '''\n        _DISTANCE = ('dot_product', 'cosine','asymmetric_cosine')\n        assert(distance_metric in _DISTANCE), 'The distance_metric has to be one of the following: dot_product, cosine, asymmetric_cosine' \n        self.k = k\n        self.data = data\n        self.csr_item_matrix = csr_item_matrix\n        if is_already_sparse == False:\n            self.create_csr_data()\n        self.distance_metric = distance_metric\n        self.alpha = alpha\n        self.similarity_item_matrix = self.fit()\n    \n    \n    \n    def create_csr_data(self):\n        '''\n        This converts the data into the sparse matrix fromat. Again, we wouldn't need this if we end up using the supporting functions!\n        '''\n        # For now, I will have to clean the data first. That is, convert the columns into desired data type and remove the unecessary ones.\n        self.data = self.data.drop(columns = ['review_id','is_read','review_text_incomplete','date_added','date_updated','read_at','started_at'])\n        self.data['user_idInt']= self.data['user_id'].str.replace('\\D+','').astype(float)\n        \n        # Here I'll create 'user_dict' & 'item_dict' such that: the indexes of the array would represent the \"key\" for the dicts.\n        self.user_dict = pd.unique(self.data['user_idInt'])\n        self.item_dict = pd.unique(self.data['book_id'])\n        \n        \n        # Then, implement the csr_mechanism and save the result in self.csr_item_matrix\n        rows = []\n        cols = []\n        data = []\n        for i in range(len(self.data)):\n            rows.append(self.user_dict.index(self.data.loc[i, 'user_idInt'])) # Here we creating the row values for csr matrix using the index values of the user_dict we created by mapping unique values in a dataframe\n            cols.append(self.item_dict.index(self.data.loc[i, 'book_id'])) # Maybe, the index part should be in square brackets as well... Will see\n            data.append(self.data.loc[i, 'rating'])\n        rows = np.array(rows)\n        cols = np.array(cols)\n        data = np.array(data)\n        self.csr_item_matrix = csr_matrix(data, (row, col))\n    \n        \n    \n    def dot_product(self, matrix):\n        '''\n        Finds the dot product similarity measure of the data\n        \n        '''\n        s = matrix * matrix.T\n        return self.top_k_items(s, self.k)\n\n        \n    def cosine(self, matrix):\n        '''\n        Finds the cosine similarity measure of the data\n        \n        '''\n        matrix2 = matrix.copy()\n        matrix2.data = np.power(matrix2.data,2)\n        X = np.power(matrix2.sum(axis=1).A1,0.5)\n        matrix_aux = (matrix * matrix.T).tocsr()\n        r, c, v = [], [], []\n        for idx1 in range(0,matrix.shape[0]):\n            for idx2 in range(matrix_aux.indptr[idx1], matrix_aux.indptr[idx1+1]):\n                row = idx1\n                col = matrix_aux.indices[idx2]\n                val = matrix_aux.data[idx2]\n                r.append(row)\n                c.append(col)\n                v.append(val/ (X[row] * X[col]))\n        s = sp.csr_matrix((v,(r,c)),shape=(matrix.shape[0],matrix.shape[0]))\n        return self.top_k_items(s, self.k)\n\n    \n    def asymmetric_cosine(matrix, alpha):\n        '''\n        Finds the assymetrical cosine similarity measure\n        \n        '''\n        matrix2 = matrix.copy()\n        matrix2.data = np.power(matrix2.data,2)\n        X = np.power(matrix2.sum(axis=1).A1,alpha)\n        Y = np.power(matrix2.sum(axis=1).A1,1-alpha)\n        matrix_aux = (matrix * matrix.T).tocsr()\n        r, c, v = [], [], []\n        for idx1 in range(0,matrix.shape[0]):\n            for idx2 in range(matrix_aux.indptr[idx1], matrix_aux.indptr[idx1+1]):\n                row = idx1\n                col = matrix_aux.indices[idx2]\n                val = matrix_aux.data[idx2]\n                r.append(row)\n                c.append(col)\n                v.append(val/ (X[row] * Y[col]))\n        s = sp.csr_matrix((v,(r,c)),shape=(matrix.shape[0],matrix.shape[0]))\n        return self.top_k_items(s, self.k)\n        \n        \n    def top_k_items(self, X, k):\n        '''\n        \n        '''\n        X = X.tocsr()\n        r, c, d = [], [], []\n        for i in range(X.shape[0]):\n            data = X.data[X.indptr[i]:X.indptr[i+1]]\n            topk = min(k, data.shape[0])\n            indices = X.indices[X.indptr[i]:X.indptr[i+1]]\n            topk_idx = np.argpartition(data, -topk)[-topk:]\n            data = data[topk_idx]\n            indices = indices[topk_idx]\n            r += np.full(topk, i).tolist()\n            c += indices.tolist()\n            d += data.tolist()\n        return sp.csr_matrix((d, (r, c)), shape=X.shape)\n        \n        \n        \n    def fit_model(self):\n        '''\n        The central function that calls the necesary functions to produce top-k recommendations\n        \n        '''\n        self.old_csr_item_matrix = self.csr_item_matrix.copy()\n        if self.distance_metric == 'dot_product':\n            similarity_scores = self.dot_product(self.csr_item_matrix)\n        elif self.distance_metric == 'cosine':\n            similarity_scores = self.cosine(self.csr_item_matrix)\n        else:\n            similarity_scores = self.asymmetric_cosine(self.csr_item_matrix, self.alpha)\n        return similarity_scores\n        \n    \n    def make_recommendations(self,user_id):\n        '''\n        \n        '''\n        self.user_id = user_id\n        self.user_id = self.user_id.str.replace('\\D+','').astype(float)\n        user_id_to_check = self.user_dict.index(self.user_id)\n        matrix = self.old_csr_item_matrix.T.tocsr()\n        indices = matrix.indices\n        indptr = matrix.indptr\n        data = matrix.data\n        book_list = []\n        for i in range(indptr[user_id_to_check], indptr[user_id_to_check + 1]):\n            book_list.append(data[i].astype(int))\n        indices = self.similarity_scores.indices\n        indptr = self.similairity_scores.indptr\n        data = self.similarity_scores.data\n        positional_increment = 0.05 # This defines the extra amount of importance added with better position in the list. So, if the last element (of a 10 item recommendation) is has a value of 1, the first element would have a value of 1 + (0.05 * 10)\n        recommendation_df = pd.DataFrame(columns = ['book_id', 'positional_value'])\n        for book in book_list:\n            top_k_recommendations = []\n            for i in range(indptr[book], indptr[book + 1]):\n                top_k_recommendations.append(data[i].astype(int))\n            start_value = 1\n            m = len(top_k_recommendations)\n            for i in range(m - 1, -1, -1):\n                book_id = top_k_recommendations[i]\n                position_value = start_value + (m - (i+1))*positional_increment\n                book_df = pd.DataFrame([book_id, positional_value], columns = ['book_id','positional_value'])\n                recommendation_df = pd.concat([recommendation_df, book_df], axis = 1)\n        recommendation_df = recommendation_df.groupby('book_id').sum()\n        recommendation_df = recommendation_df.sort_values('positional_value')[:self.k]\n        self.recommendations = np.array(recommendation_df.loc[:,'book_id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Using Sparse Matrices and Similaripy Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I can even include the train test split and evaluate the model's efficacy using RMSE, but for that, I will have to figure out how to extract similarity measures from \n# the implicit library, and then transform those similarity measures using \nclass knn_similaripy():\n    def __init__(self, normalization_type = None, tf_type= 'sqrt', idf_type = 'bm25', distance_metric = 'cosine',\n                 user_id = None, data = None, csr_item_matrix = None, k = 10, is_already_sparse = False,\n                 create_ratings = False):\n        '''\n        This class produces recommendations based on Nearest Neighbour mechanism. We will use the KNN implementation of implicit library to perform this task.\n        \n        Args:\n            data (DataFrame) -> Actual DataFrame containing three columns - 'book_id', 'user_id', 'rating'\n            user_id (Int) -> User Id of the user for which we want to make the recommendations\n            k (int) -> Number of recommendations we want to produce\n        \n        Returns:\n            recommendations (iterable) -> A list/array of item_id, of the books being recommended.\n        \n        '''\n        _NORMALIZATIONS = ('l1', 'l2', 'max')\n        _TF = ('binary', 'raw', 'sqrt', 'freq', 'log')\n        _IDF = ('unary', 'base', 'smooth', 'prob', 'bm25', 'bm25plus')\n        _DISTANCE_METRIC = ('dot_product', 'cosine', 'assymetric_cosine','jaccard','dice','tversky','p3alpha','rp3beta')\n        assert(normalization_type in _NORMALIZATIONS), 'Normalization functions can only be l1, l2, max'\n        assert(tf_type in _TF), 'Term Frequency functions can only be binary, raw, sqrt, freq, log'\n        assert(idf_type in _IDF), 'Inverse Document Frequency functions can only be one of unary, base, smooth, prob, bm25, bm25plus'\n        assert(distance_metric in _DISTANCE_METRIC), 'Distance Metric must be one of the many prescribed by Similaripy'\n        self.user_id = user_id\n        self.data = data\n        self.k = k\n        self.csr_item_matrix = csr_item_matrix # This will house the sparse matrix of the original data (if passed data isn't csr_matrix already)\n        self.is_already_sparse = is_already_sparse\n        if not self.is_already_sparse and data != None:\n            self.create_csr_data()\n        self.normalization_type = normalization_type\n        self.tf_type = tf_type\n        self.idf_type = idf_type\n        self.distance_metric = distance_metric\n        self.recommendations = None\n        self.ratings_matrix = None # Here I will store the predicted ratings for all user item pairs. Special care will have to be taken to ensure that this does not excede memory.\n        self.create_ratings = create_ratings\n        self.train_data = None # Here I will store the train and test data, respectively\n        self.test_data = None\n        if self.create_ratings:\n            self.test_train_split()\n        self.similarity_scores = self.fit_model()\n        \n        \n        \n    def create_csr_data(self):\n        '''\n        This function converts the original dataframe into a sparse matrix.\n        \n        '''\n        # For now, I will have to clean the data first. That is, convert the columns into desired data type and remove the unecessary ones.\n        self.data = self.data.drop(columns = ['review_id','is_read','review_text_incomplete','date_added','date_updated','read_at','started_at'])\n        self.data['user_idInt']= self.data['user_id'].str.replace('\\D+','').astype(float)\n        \n        # Here I'll create 'user_dict' & 'item_dict' such that: the indexes of the array would represent the \"key\" for the dicts.\n        self.user_dict = pd.unique(self.data['user_idInt'])\n        self.item_dict = pd.unique(self.data['book_id'])\n        \n        \n        # Then, implement the csr_mechanism and save the result in self.csr_item_matrix\n        rows = []\n        cols = []\n        data = []\n        for i in range(len(self.data)):\n            rows.append(self.user_dict.index(self.data.loc[i, 'user_idInt'])) # Here we creating the row values for csr matrix using the index values of the user_dict we created by mapping unique values in a dataframe\n            cols.append(self.item_dict.index(self.data.loc[i, 'book_id'])) # Maybe, the index part should be in square brackets as well... Will see\n            data.append(self.data.loc[i, 'rating'])\n        rows = np.array(rows)\n        cols = np.array(cols)\n        data = np.array(data)\n        self.csr_item_matrix = csr_matrix(data, (row, col))\n    \n    \n    def fit_model(self):\n        '''\n        This function fits the Similaripy model to the given data, and creates the 'similarity_matrix' for all the items.\n        \n        First I'll normalize the data and tranform with the appropriate tf_idf transform.\n        Then, I'll feed the data with the specific distance metric and produce the similarity matrix\n        \n        \n        '''\n        self.old_csr_item_matrix = self.csr_item_matrix.copy()\n        # Normalization part\n        if self.normalization_type == None:\n            continue\n        else:\n            self.csr_item_matrix = normalize(self.csr_item_matrix, axis = 1, norm = self.normalization_type)\n            \n        # Tf_IDf part\n        if self.tf_type == None and self.idf_type == None:\n            continue\n        elif self.idf_type == 'bm25':\n            self.csr_item_matrix = bm25(self.csr_item_matrix, tf_mode = self.tf_type, idf_mode = self.idf_type)\n        elif self.idf_type == 'bm25plus':\n            self.csr_item_matrix = bm25plus(self.csr_item_matrix, tf_mode = self.tf_type, idf_mode = self.idf_type)\n        else:\n            self.csr_item_matrix = tfidf(self.csr_item_matrix, tf_mode = self.tf_type, idf_mode = self.idf_type)\n        \n        # Fitting the model (!!!Still have to figure out if this returns the a csr matrix with top k items for each item or not!!!)\n        if self.distance_metric == 'dot_product':\n            model = sim.dot_product(self.csr_item_matrix, k = self.k)\n        elif self.distance_metric == 'cosine':\n            model = sim.cosine(self.csr_item_matrix, k = self.k)\n        elif self.distance_metric == 'assymetric_cosine':\n            model = sim.assymetric_cosine(self.csr_item_matrix, alpha = 0.2, k = self.k)\n        elif self.distance_metric == 'jaccard':\n            model = sim.jaccard(self.csr_item_matrix, k = self.k)\n        elif self.distance_metric == 'dice':\n            model = sim.dice(self.csr_item_matrix, k = self.k)\n        elif self.distance_metric == 'tversky':\n            model = sim.tversky(self.csr_item_matrix, alpha = 0.8, beta = 0.4, k = self.k)\n        elif self.distance_metric == 'p3alpha':\n            model = sim.p3alpha(self.csr_item_matrix, alpha = 0.8, k = self.k)\n        else:\n            model = sim.rp3beta(self.csr_item_matrix, alpha = 0.8, beta = 0.4, k = self.k)\n        \n        return model # Here I am assuming that the model creates similarity scores of top-k items for each item in self.similarity_scores (which it most probably does!)\n    \n    \n    def create_recommendations(self, user_id = 1, positional_increment = 0.05):\n        '''\n        This function covers the utility aspect of the recommendation systems, by making book recommendations for the given user_id, by making sure that the user history is \n        taken into account.\n        \n        Since, this model produces similarity scores of the items, producing recommendations for a specific user has to do the following task:\n        It produces top-K similar books for each of the items that the user has rated. It then produces a normalized score of those recommendations (based on user rating).\n        The top-K books thus produced are then recommended to the user.\n        \n        Here, I will have to decide if I want to implement that normalization approach where the baseline ratings are removed from the rating_score being generated,\n        to account for the user bias (some users are prone to giving too good or too bad ratings)\n        \n        Args:\n            user_id (object/int) -> User_id either in form given in the dataset or in integer form\n            positional_increment (float) -> Defines the extra bit of importance we give to having a single position advantage in recommendations\n        \n        '''\n        self.user_id = user_id\n        self.user_id = self.user_id.str.replace('\\D+','').astype(float)\n        user_id_to_check = self.user_dict.index(self.user_id)\n        matrix = self.old_csr_item_matrix.T.tocsr()\n        indices = matrix.indices\n        indptr = matrix.indptr\n        data = matrix.data\n        book_list = []\n        for i in range(indptr[user_id_to_check], indptr[user_id_to_check + 1]):\n            book_list.append(data[i].astype(int))\n        indices = self.similarity_scores.indices\n        indptr = self.similairity_scores.indptr\n        data = self.similarity_scores.data\n        positional_increment = 0.05 # This defines the extra amount of importance added with better position in the list. So, if the last element (of a 10 item recommendation) is has a value of 1, the first element would have a value of 1 + (0.05 * 10)\n        recommendation_df = pd.DataFrame(columns = ['book_id', 'positional_value'])\n        for book in book_list:\n            top_k_recommendations = []\n            for i in range(indptr[book], indptr[book + 1]):\n                top_k_recommendations.append(data[i].astype(int))\n            start_value = 1\n            m = len(top_k_recommendations)\n            for i in range(m - 1, -1, -1):\n                book_id = top_k_recommendations[i]\n                position_value = start_value + (m - (i+1))*positional_increment\n                book_df = pd.DataFrame([book_id, positional_value], columns = ['book_id','positional_value'])\n                recommendation_df = pd.concat([recommendation_df, book_df], axis = 1)\n        recommendation_df = recommendation_df.groupby('book_id').sum()\n        recommendation_df = recommendation_df.sort_values('positional_value')[:self.k]\n        self.recommendations = np.array(recommendation_df.loc[:,'book_id'])\n    \n    \n    def test_train_split(self):\n        '''\n        This function will only be used if we are gonna produce ratings as well, along with recommendations. The main problem with this approach would be that we can \n        only create ratings for the items that do fall into the 'top-K' similar items for each item, and there are high chances that the user might not have actually rated\n        any/all of the books that we are creating the ratings for. \n        So for a sufficiently large enough test set, the number actual test instances that we can compare our model's prediction capability would be actually quite small.\n        \n        '''\n        \n        \n        \n        \n        \n        \n    def create_ratings(self):\n        '''\n        This function will convert the similarity values, (derived from fit_model() function) using the approach defined in 'Neigborhood meets Factorization' paper\n        \n        \n        '''\n        \n        \n        \n        \n        \n        \n    def efficacy_score(self):\n        '''\n        Using the test train split, and after creating the ratings, this function will produce the error metric on recommendations using RMSE score as the deciding metric.\n        This would allow us to compare various models amongst each other\n        \n        '''\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"poetry_nn_df1 = interactions_df.copy()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Shortcomings still remaining:\n* Point 1\n* Point 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 2: Matrix Factorization Method"},{"metadata":{},"cell_type":"markdown","source":"## Surprise Implementation"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Creating Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the data into the format Surprise Requires it in\nreader = Reader(rating_scale=(1, 5))\ndata_surprise = Dataset.load_from_df(interactions_df[['user_idInt', 'book_id', 'rating']], reader)\n# Creating train and test splits, with 10 percent of data being saved for \ntraining_data, testing_data = train_test_split(data_surprise, test_size=.10)\n\n","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Defining Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"algo1 = SVD(n_factors = 20, n_epochs = 10, verbose = True)\nalgo1.fit(training_data)","execution_count":15,"outputs":[{"output_type":"stream","text":"Processing epoch 0\nProcessing epoch 1\nProcessing epoch 2\nProcessing epoch 3\nProcessing epoch 4\nProcessing epoch 5\nProcessing epoch 6\nProcessing epoch 7\nProcessing epoch 8\nProcessing epoch 9\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f900f7a9210>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we try to increase the number of factors to 50, and number of epochs to 30\nalgo2 = SVD(n_factors = 50, n_epochs = 30, verbose = True)\nalgo2.fit(training_data)","execution_count":16,"outputs":[{"output_type":"stream","text":"Processing epoch 0\nProcessing epoch 1\nProcessing epoch 2\nProcessing epoch 3\nProcessing epoch 4\nProcessing epoch 5\nProcessing epoch 6\nProcessing epoch 7\nProcessing epoch 8\nProcessing epoch 9\nProcessing epoch 10\nProcessing epoch 11\nProcessing epoch 12\nProcessing epoch 13\nProcessing epoch 14\nProcessing epoch 15\nProcessing epoch 16\nProcessing epoch 17\nProcessing epoch 18\nProcessing epoch 19\nProcessing epoch 20\nProcessing epoch 21\nProcessing epoch 22\nProcessing epoch 23\nProcessing epoch 24\nProcessing epoch 25\nProcessing epoch 26\nProcessing epoch 27\nProcessing epoch 28\nProcessing epoch 29\n","name":"stdout"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f900f7a96d0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"algo3 = SVD(n_factors = 100, n_epochs = 30, lr_all = 0.0075, reg_all = 0.03, verbose = True)\nalgo3.fit(training_data)","execution_count":17,"outputs":[{"output_type":"stream","text":"Processing epoch 0\nProcessing epoch 1\nProcessing epoch 2\nProcessing epoch 3\nProcessing epoch 4\nProcessing epoch 5\nProcessing epoch 6\nProcessing epoch 7\nProcessing epoch 8\nProcessing epoch 9\nProcessing epoch 10\nProcessing epoch 11\nProcessing epoch 12\nProcessing epoch 13\nProcessing epoch 14\nProcessing epoch 15\nProcessing epoch 16\nProcessing epoch 17\nProcessing epoch 18\nProcessing epoch 19\nProcessing epoch 20\nProcessing epoch 21\nProcessing epoch 22\nProcessing epoch 23\nProcessing epoch 24\nProcessing epoch 25\nProcessing epoch 26\nProcessing epoch 27\nProcessing epoch 28\nProcessing epoch 29\n","name":"stdout"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f900f7a9e50>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Evaluating RMSE scores of test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = algo1.test(testing_data)\nprint(predictions[0:5])\npredicted_values = [prediction[3] for prediction in predictions]\nactual_values = [prediction[2] for prediction in predictions]\nscore = mean_squared_error(predicted_values, actual_values)\nprint('Mean Squared Error for Algo1 is ', score)\n","execution_count":18,"outputs":[{"output_type":"stream","text":"[Prediction(uid=5.2980622290063354e+17, iid=1715, r_ui=4.0, est=4.149277036408901, details={'was_impossible': False}), Prediction(uid=3.571451243629403e+22, iid=883196, r_ui=5.0, est=4.152715398287774, details={'was_impossible': False}), Prediction(uid=5994620866580826.0, iid=38154, r_ui=2.0, est=4.085838519243364, details={'was_impossible': False}), Prediction(uid=7.563903201718603e+27, iid=2335333, r_ui=4.0, est=3.9173706026540316, details={'was_impossible': False}), Prediction(uid=8.352998418088603e+16, iid=17996971, r_ui=5.0, est=3.610329563036342, details={'was_impossible': False})]\nMean Squared Error for Algo1 is  0.7615491615666556\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(predicted_values)","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"122906"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = algo2.test(testing_data)\nprint(predictions[0:5])\npredicted_values = [prediction[3] for prediction in predictions]\nactual_values = [prediction[2] for prediction in predictions]\nscore = mean_squared_error(predicted_values, actual_values)\nprint('Mean Squared Error for Algo1 is ', score)","execution_count":20,"outputs":[{"output_type":"stream","text":"[Prediction(uid=5.2980622290063354e+17, iid=1715, r_ui=4.0, est=4.335944078155905, details={'was_impossible': False}), Prediction(uid=3.571451243629403e+22, iid=883196, r_ui=5.0, est=4.093212079531559, details={'was_impossible': False}), Prediction(uid=5994620866580826.0, iid=38154, r_ui=2.0, est=4.144689621439386, details={'was_impossible': False}), Prediction(uid=7.563903201718603e+27, iid=2335333, r_ui=4.0, est=3.9531251142951223, details={'was_impossible': False}), Prediction(uid=8.352998418088603e+16, iid=17996971, r_ui=5.0, est=3.493373375495529, details={'was_impossible': False})]\nMean Squared Error for Algo1 is  0.7434132015562548\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = algo3.test(testing_data)\nprint(predictions[0:5])\npredicted_values = [prediction[3] for prediction in predictions]\nactual_values = [prediction[2] for prediction in predictions]\nscore = mean_squared_error(predicted_values, actual_values)\nprint('Mean Squared Error for Algo1 is ', score)","execution_count":21,"outputs":[{"output_type":"stream","text":"[Prediction(uid=5.2980622290063354e+17, iid=1715, r_ui=4.0, est=4.48219761839029, details={'was_impossible': False}), Prediction(uid=3.571451243629403e+22, iid=883196, r_ui=5.0, est=4.050519918379341, details={'was_impossible': False}), Prediction(uid=5994620866580826.0, iid=38154, r_ui=2.0, est=4.2968694049365315, details={'was_impossible': False}), Prediction(uid=7.563903201718603e+27, iid=2335333, r_ui=4.0, est=3.911337918606041, details={'was_impossible': False}), Prediction(uid=8.352998418088603e+16, iid=17996971, r_ui=5.0, est=3.8544139248128486, details={'was_impossible': False})]\nMean Squared Error for Algo1 is  0.7447869159851056\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"##### Result Evaluation and comparison of various models\n* Increasing the number of factors and number of epochs helped improve the MSE score.\n* Increasing the learning rate and relaxing the regularization rate actually detoriated the performance of the algorithm a bit"},{"metadata":{},"cell_type":"markdown","source":"### Producing Recommendations\n\nAfter training the model with the Surprise implementation of SVD, we could produce the recommendations using the following metric:\n* Simple rating values of all the items for a given user, and then print the top 10 from the sorted list\n* Rating values of only the top-k items recommended by the item similarity algorithm, and printing them out. This would allow us to compare the efficacy of different models."},{"metadata":{},"cell_type":"markdown","source":"###### 1. Predict Ratings of ALL items, and then produce the top-K recommendations from them"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n(predictions, n=10):\n    '''Return the top-N recommendation for each user from a set of predictions.\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    '''\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n\n\n\ndef get_top_n_for_user(raw_user_id, algorithm_object = algo1, n = 10, fill = None):\n    '''\n    This function produces the top_n recommendations for the given user_id\n    \n    Args:\n        raw_user_id (float) -> Takes the user_id in the raw (original format)\n        n (int) -> Number of top items to be recommended\n        fill (float) -> Temporary rating value to be filled (of no practical use to our current problem)\n    \n    '''\n    # Converts the raw_id into inner_uid\n    inner_uid = training_data.to_inner_uid(raw_user_id)\n    \n    # Collects all the items that the user has already rated\n    user_items = set([j for (j, _) in training_data.ur[inner_uid]])\n    \n    # Creates the anti-testset specific to that particular user\n    anti_testset = []\n    fill = training_data.global_mean if fill is None else float(fill)\n    anti_testset += [(training_data.to_raw_uid(inner_uid), training_data.to_raw_iid(i), fill) for\n                             i in training_data.all_items() if\n                             i not in user_items]\n    \n    \n    # Produces the predictions using the algorithm of choice\n    predictions = algorithm_object.test(anti_testset)\n    \n    \n    # Produces the top-n recommendations from that prediction set\n    top_n = get_top_n(predictions, n)\n    return top_n, predictions","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_user_id = 1.3895397206716666e+21 # Some random user_idInt\nalgorithm_object = algo2 # Any trained model of choice\n\ntop_n, predictions = get_top_n_for_user(test_user_id, algorithm_object, n = 20)\n#testset = training_data.build_anti_testset() # Actually, probably this step takes even more memory space. Because if the code had ended executing this part, then it would have returned a name error for the next line, because there is not variable named 'algo'\n#predictions = algo.test(testset) # This step (with the Surprise implementation, take a LOOTT of memory)\n#top_n = get_top_n(predictions, n=10)\n\n# Print the recommended items for each user\nfor uid, user_ratings in top_n.items():\n    topn_items = [iid for (iid, _) in user_ratings]\n    print(uid, [iid for (iid, _) in user_ratings])\n\n","execution_count":23,"outputs":[{"output_type":"stream","text":"1.3895397206716666e+21 [6716521, 3289921, 46287, 2142496, 22305809, 8763495, 22736736, 17227753, 29599121, 31945135, 23577568, 498736, 12777588, 1108687, 22295730, 18459455, 6965880, 4759649, 13123245, 26196150]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the item Ids into Id form of book_df\n\nfor i in range(len(topn_items)):\n    topn_items[i] = training_data.to_inner_iid(topn_items[i])","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's now show the book imaegs for the recommended books\nprint(topn_items)\nprint_book_images(book_df = book_df, recommendations = topn_items)","execution_count":25,"outputs":[{"output_type":"stream","text":"[673, 2331, 2348, 2471, 4656, 4770, 7440, 7961, 13004, 17532, 18364, 24625, 5247, 2914, 327, 6598, 23034, 11562, 356, 4838]\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"Empty DataFrame\nColumns: [isbn, text_reviews_count, series, country_code, language_code, popular_shelves, asin, is_ebook, average_rating, kindle_asin, similar_books, description, format, link, authors, publisher, num_pages, publication_day, isbn13, publication_month, edition_information, publication_year, url, image_url, book_id, ratings_count, work_id, title, title_without_series]\nIndex: []\n\n[0 rows x 29 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>isbn</th>\n      <th>text_reviews_count</th>\n      <th>series</th>\n      <th>country_code</th>\n      <th>language_code</th>\n      <th>popular_shelves</th>\n      <th>asin</th>\n      <th>is_ebook</th>\n      <th>average_rating</th>\n      <th>kindle_asin</th>\n      <th>...</th>\n      <th>publication_month</th>\n      <th>edition_information</th>\n      <th>publication_year</th>\n      <th>url</th>\n      <th>image_url</th>\n      <th>book_id</th>\n      <th>ratings_count</th>\n      <th>work_id</th>\n      <th>title</th>\n      <th>title_without_series</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 29 columns</p>\n</div>"},"metadata":{}},{"output_type":"stream","text":"Series([], Name: image_url, dtype: object)\n","name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"expected str, bytes or os.PathLike object, not Series","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-e0aa4078080a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's now show the book imaegs for the recommended books\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopn_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint_book_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecommendations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopn_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-34ebdbabadc7>\u001b[0m in \u001b[0;36mprint_book_images\u001b[0;34m(book_df, recommendations)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mauthors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbook\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'authors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Image(requests.get(image_url).content,width=width_pixels, height=height_pixels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth_pixels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight_pixels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Title : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Authors : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m             \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No image data found. Expecting filename, url, or data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m_find_ext\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_find_ext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m         \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/posixpath.py\u001b[0m in \u001b[0;36msplitext\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not Series"]}]},{"metadata":{},"cell_type":"markdown","source":"###### 2. Predicted ratings for the top-K most similar items"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will implement this part after Part 1 code successfully implements\n\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Shortcomings still remaining:\n* It does not use sparse matrices. Therefore making it tough for working on the big files I have.\n* While it has a 'dump' function for downloading the trained model and loading it later, it does not have iterative training. Therefore, it will have to train the model from scratch each time, and is not usefule for online recommenders.\n* It does not have an option for trying out ALS implementation of MF. (This has been implemented in the next part)\n* The SVD implementation of Surprise cannot accomodate implicit data. (This has been done in the next part)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 3: Weighted Regularized Matrix Factorization"},{"metadata":{},"cell_type":"markdown","source":"Questions to answer:\n* Q. Whether multiple coloumns of implicit data can be taken in by SVDpp? \n\n**Answer:** SVDpp inherently produces implicit data in form of 0/1 representation for whether the user has rated the item or not. This doesn't provide much new information to the model, and is the reason why the model doesn't improve by much\n* Q. Can it use temporal implicit information? Like when did the user start reading the book? When did he finish it? How long did it take the user to finish it? etc.\n\n**Answer:** No, it the current implementations don't make use of temporal features explicitly. However, they are being employed implicitly in section 2 of Implicit implementation\n* Q. Can it handle implicit data in format other than one-hot encoding?\n\n**Answer:** Implicit implementation can handle any form of implicit feedback as long as it is being fed in form of numerical data (weights)."},{"metadata":{},"cell_type":"markdown","source":"##### I will try to implement the following:\n* The surprise implementation of Koren:2008 SVDpp implementation\n* Implicit implementation of SVD (Which most probably just uses the implicit data)"},{"metadata":{},"cell_type":"markdown","source":"## 1. Surprise Implementation"},{"metadata":{},"cell_type":"markdown","source":"##### Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the data into the format Surprise Requires it in\nreader = Reader(rating_scale=(1, 5))\ndata_surprise = Dataset.load_from_df(interactions_df[['user_idInt', 'book_id', 'rating']], reader)\n# Creating train and test splits, with 10 percent of data being saved for \ntraining_data, testing_data = train_test_split(data_surprise, test_size=.10)","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Fitting Models\n\nThis model simply uses a 0/1 form of implicit data which simply implies whether the user has rated the item or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"algo4 = SVDpp(n_factors = 20, n_epochs = 10, verbose = True)\nalgo4.fit(training_data)","execution_count":27,"outputs":[{"output_type":"stream","text":" processing epoch 0\n processing epoch 1\n processing epoch 2\n processing epoch 3\n processing epoch 4\n processing epoch 5\n processing epoch 6\n processing epoch 7\n processing epoch 8\n processing epoch 9\n","name":"stdout"},{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"<surprise.prediction_algorithms.matrix_factorization.SVDpp at 0x7f8fe55e1a90>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"##### Evaluation on Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = algo4.test(testing_data)\nprint(predictions[0:5])\npredicted_values = [prediction[3] for prediction in predictions]\nactual_values = [prediction[2] for prediction in predictions]\nscore = mean_squared_error(predicted_values, actual_values)\nprint('Mean Squared Error for Algo4 is ', score)","execution_count":28,"outputs":[{"output_type":"stream","text":"[Prediction(uid=6.138830758441367e+19, iid=1420, r_ui=3.0, est=4.125915776356357, details={'was_impossible': False}), Prediction(uid=7.804568319119688e+18, iid=57903, r_ui=3.0, est=3.6323405123587427, details={'was_impossible': False}), Prediction(uid=9.866883270058431e+23, iid=3280282, r_ui=4.0, est=3.810850840157827, details={'was_impossible': False}), Prediction(uid=9.991033644955132e+21, iid=728862, r_ui=3.0, est=4.016594605145156, details={'was_impossible': False}), Prediction(uid=7.472232554643305e+18, iid=66443, r_ui=5.0, est=4.101214265380013, details={'was_impossible': False})]\nMean Squared Error for Algo4 is  0.7345422169990495\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"##### Producing Recommendations"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_user_id = 1.3895397206716666e+21 # Some random user_idInt\nalgorithm_object = algo4 # Any trained model of choice\n\ntop_n, predictions = get_top_n_for_user(test_user_id, algorithm_object, n = 20)\n#testset = training_data.build_anti_testset() # Actually, probably this step takes even more memory space. Because if the code had ended executing this part, then it would have returned a name error for the next line, because there is not variable named 'algo'\n#predictions = algo.test(testset) # This step (with the Surprise implementation, take a LOOTT of memory)\n#top_n = get_top_n(predictions, n=10)\n\n# Print the recommended items for each user\nfor uid, user_ratings in top_n.items():\n    topn_items = [iid for (iid, _) in user_ratings]\n    print(uid, [iid for (iid, _) in user_ratings])\n","execution_count":29,"outputs":[{"output_type":"stream","text":"1.3895397206716666e+21 [18144783, 230231, 1036552, 1108687, 1416, 22204746, 29599121, 10787311, 230246, 18459455, 24137295, 17227753, 30117, 2014000, 22295730, 83267, 46287, 46291, 2142496, 1079359]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2. Implicit Implementation\n\nThe implementation I am gonna primarily try out is LMF (Logistic Matrix Factorization).Its a slightly advanced version of general matrix factorization algorithm as described in [this paper](https://web.stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf). If time allows for it, I will also try out the ALS implementation and document the differences in results and computational complexity\nThis implementation works solely on 'weights' between user and item interactions. That is, it doesn't account for ratings, instead only accounts for implicit feedback being fed in form of 'weights'. I will try out the following two things in this module:\n\n* Section 1: Simply use ratings as weights\n* Section 2: Use just the implicit data from our interactions in form of weights and produce recommendations\n* Section 3: Create a weight metric including both the implicit feedback and the explicit feedback (something like described by Koren et al.), and then train the same LFM model"},{"metadata":{},"cell_type":"markdown","source":"### Section 1: Ratings as weights"},{"metadata":{},"cell_type":"markdown","source":"##### Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"display(interactions_df.tail())\ndisplay(len(interactions_df))\n\ninteractions_df.index = pd.RangeIndex(len(interactions_df.index))\ndisplay(interactions_df.tail())\ndisplay(len(interactions_df))","execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"          book_id  rating    user_idInt\n2734340      2547       5  1.484971e+21\n2734341  11047097       4  6.951628e+21\n2734343   7433930       5  5.948671e+17\n2734348  16170625       5  5.948671e+17\n2734349  16101638       4  5.948671e+17","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>rating</th>\n      <th>user_idInt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2734340</th>\n      <td>2547</td>\n      <td>5</td>\n      <td>1.484971e+21</td>\n    </tr>\n    <tr>\n      <th>2734341</th>\n      <td>11047097</td>\n      <td>4</td>\n      <td>6.951628e+21</td>\n    </tr>\n    <tr>\n      <th>2734343</th>\n      <td>7433930</td>\n      <td>5</td>\n      <td>5.948671e+17</td>\n    </tr>\n    <tr>\n      <th>2734348</th>\n      <td>16170625</td>\n      <td>5</td>\n      <td>5.948671e+17</td>\n    </tr>\n    <tr>\n      <th>2734349</th>\n      <td>16101638</td>\n      <td>4</td>\n      <td>5.948671e+17</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1229059"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"          book_id  rating    user_idInt\n1229054      2547       5  1.484971e+21\n1229055  11047097       4  6.951628e+21\n1229056   7433930       5  5.948671e+17\n1229057  16170625       5  5.948671e+17\n1229058  16101638       4  5.948671e+17","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>rating</th>\n      <th>user_idInt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1229054</th>\n      <td>2547</td>\n      <td>5</td>\n      <td>1.484971e+21</td>\n    </tr>\n    <tr>\n      <th>1229055</th>\n      <td>11047097</td>\n      <td>4</td>\n      <td>6.951628e+21</td>\n    </tr>\n    <tr>\n      <th>1229056</th>\n      <td>7433930</td>\n      <td>5</td>\n      <td>5.948671e+17</td>\n    </tr>\n    <tr>\n      <th>1229057</th>\n      <td>16170625</td>\n      <td>5</td>\n      <td>5.948671e+17</td>\n    </tr>\n    <tr>\n      <th>1229058</th>\n      <td>16101638</td>\n      <td>4</td>\n      <td>5.948671e+17</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1229059"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First, let's identify the unique rows and columns\nuser_dict = pd.unique(interactions_df['user_idInt'])\nitem_dict = pd.unique(interactions_df['book_id'])\n\n\n# Then, implement the csr_mechanism and save the result in self.csr_item_matrix\nrows = []\ncols = []\ndata = []\nfor i in range(len(interactions_df)):\n    #print(interactions_df.loc[i, 'user_idInt'])\n    cols.append(np.where(user_dict == interactions_df.loc[i, 'user_idInt']))\n    #cols.append(user_dict.index(interactions_df.loc[i, 'user_idInt'])) # Here we creating the row values for csr matrix using the index values of the user_dict we created by mapping unique values in a dataframe\n    rows.append(np.where(item_dict == interactions_df.loc[i, 'book_id']))\n    #rows.append(item_dict.index(interactions_df.loc[i, 'book_id'])) # Maybe, the index part should be in square brackets as well... Will see\n    data.append(interactions_df.loc[i, 'rating'])\nrows = np.array(rows)\ncols = np.array(cols)\ndata = np.array(data)\n\n","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = np.array(rows).ravel()\ncols = np.array(cols).ravel()\ndata = np.array(data).ravel()\nprint(len(rows))\nprint(len(cols))\nitem_users = coo_matrix((data, (rows, cols)), shape = (1229059, 1229059))","execution_count":32,"outputs":[{"output_type":"stream","text":"1229059\n1229059\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"##### Model Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1_1 = implicit.lmf.LogisticMatrixFactorization(factors=20)\nmodel_1_1.fit(item_users)","execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0c41a1db5a43a994d0d20c8dbae33e"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1_2 = implicit.lmf.LogisticMatrixFactorization(factors=50)\nmodel_1_2.fit(item_users)","execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e950c412318461d98614e6d163af3ab"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_1_3 = implicit.als.AlternatingLeastSquares(factors=30, num_threads = 0)\nmodel_1_3.fit(item_users)","execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52ce186274144e5bb0cdab94d4a4e033"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"##### Producing Recommendations"},{"metadata":{"trusted":true},"cell_type":"code","source":"# userid = 1.3895397206716666e+21 # Some random user_idInt\nuser_id = np.where(user_dict == interactions_df.loc[0, 'user_idInt'])# Some random user_idInt\nprint(user_id[0][0])\nuser_id = user_id[0][0]\nuser_items = item_users.T.tocsr()\nrecommendations = model_1_1.recommend(user_id, user_items)\nprint(recommendations)","execution_count":50,"outputs":[{"output_type":"stream","text":"0\n[(383, 5.4318256), (2473, 4.5945945), (1046, 4.590869), (32, 4.565134), (582, 4.4511876), (3820, 4.3986597), (1106, 4.313041), (1101, 4.2043433), (51, 4.0418024), (373, 4.0374227)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#userid = 1.3895397206716666e+21 # Some random user_idInt\nuser_id = np.where(user_dict == interactions_df.loc[0, 'user_idInt'])# Some random user_idInt\n#print(user_id[0][0])\nuser_id = user_id[0][0]\nuser_items = item_users.T.tocsr()\nrecommendations = model_1_2.recommend(user_id, user_items)\nprint(recommendations)","execution_count":51,"outputs":[{"output_type":"stream","text":"[(554, 2.9395614), (579, 2.8796337), (404, 2.7758117), (1367, 2.2736778), (936, 2.2169206), (130, 2.171403), (80, 2.1360292), (33, 2.1199784), (172, 2.1137226), (156, 2.0383797)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_id = np.where(user_dict == interactions_df.loc[0, 'user_idInt'])# Some random user_idInt\nprint(user_id[0][0])\nuser_id = user_id[0][0]\nuser_items = item_users.T.tocsr()\nrecommendations = model_1_3.recommend(user_id, user_items)\nprint(recommendations)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Section 2: Just implicit feedback data"},{"metadata":{},"cell_type":"markdown","source":"##### Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 4: Neural Collaborative Filtering (Optional)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}