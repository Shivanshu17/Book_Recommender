{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/useriteminteractionspoetry/interactions_poetry.csv\n/kaggle/input/goodreads-interactions-poetry/goodreads_interactions_poetry.json\n/kaggle/input/goodbooks-books-poetry/goodreads_books_poetry.json\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install similaripy\n!pip install scikit-surprise","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting similaripy\n  Downloading similaripy-0.1.1.tar.gz (334 kB)\n\u001b[K     |████████████████████████████████| 334 kB 6.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from similaripy) (1.4.1)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from similaripy) (1.18.1)\nRequirement already satisfied: tqdm>=4.19.6 in /opt/conda/lib/python3.7/site-packages (from similaripy) (4.45.0)\nBuilding wheels for collected packages: similaripy\n  Building wheel for similaripy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for similaripy: filename=similaripy-0.1.1-cp37-cp37m-linux_x86_64.whl size=1515867 sha256=65602ff3fa0ab857468aff5773c3082ebcd987cdb5248a53f6810c68f1e970e5\n  Stored in directory: /root/.cache/pip/wheels/52/77/1c/8da70f5b02be559b78118a9867c2fd864ed1d221d3d440ee95\nSuccessfully built similaripy\nInstalling collected packages: similaripy\nSuccessfully installed similaripy-0.1.1\nRequirement already satisfied: scikit-surprise in /opt/conda/lib/python3.7/site-packages (1.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (0.14.1)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (1.14.0)\nRequirement already satisfied: numpy>=1.11.2 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (1.18.1)\nRequirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-surprise) (1.4.1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nfrom pandas.io.json import json_normalize\nfrom sys import getsizeof\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nfrom scipy.sparse import csr_matrix, csc_matrix, coo_matrix\nimport scipy.sparse as sp\nfrom scipy import *\n\nimport similaripy as sim\nfrom similaripy.normalization import normalize, bm25, bm25plus, tfidf\n\nfrom implicit.nearest_neighbours import ItemItemRecommender, bm25_weight, tfidf_weight\nimport implicit\n\nfrom surprise import SVD\nfrom surprise import SVDpp\nfrom surprise.model_selection import cross_validate\nfrom surprise import BaselineOnly\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise.model_selection import train_test_split\n\n# For displaying images\nfrom IPython.display import Image \nfrom matplotlib.pyplot import figure, imshow, axis\nfrom matplotlib.image import imread\n","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Data\n\nThe following two cells are temporary, and will be removed once we have the interactions and book data in the required formats"},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions_df = pd.read_csv('/kaggle/input/useriteminteractionspoetry/interactions_poetry.csv')\ndisplay(interactions_df.head(5))\ndisplay(interactions_df.describe())\n#display(interactions_df.info)","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"   Unnamed: 0                           user_id   book_id  \\\n0           0  8842281e1d1347389f2ab93d60773d4d      1384   \n1           1  8842281e1d1347389f2ab93d60773d4d      1376   \n2           2  8842281e1d1347389f2ab93d60773d4d     30119   \n3           3  72fb0d0087d28c832f15776b0d936598  24769928   \n4           4  72fb0d0087d28c832f15776b0d936598     30119   \n\n                          review_id  is_read  rating review_text_incomplete  \\\n0  1bad0122cebb4aa9213f9fe1aa281f66     True       4                    NaN   \n1  eb6e502d0c04d57b43a5a02c21b64ab4     True       4                    NaN   \n2  787564bef16cb1f43e0f641ab59d25b7     True       5                    NaN   \n3  8c80ee74743d4b3b123dd1a2e0c0bcac    False       0                    NaN   \n4  2a83589fb597309934ec9b1db5876aaf     True       3                    NaN   \n\n                       date_added                    date_updated  \\\n0  Wed May 09 09:33:44 -0700 2007  Wed May 09 09:33:44 -0700 2007   \n1  Wed May 09 09:33:18 -0700 2007  Wed May 09 09:33:18 -0700 2007   \n2  Sat Jan 13 13:44:20 -0800 2007  Wed Mar 22 11:45:08 -0700 2017   \n3  Wed Apr 27 11:05:51 -0700 2016  Wed Apr 27 11:05:52 -0700 2016   \n4  Mon Jun 04 18:58:08 -0700 2012  Mon Jun 04 18:58:13 -0700 2012   \n\n                          read_at started_at  \n0                             NaN        NaN  \n1                             NaN        NaN  \n2  Tue Mar 01 00:00:00 -0800 1983        NaN  \n3                             NaN        NaN  \n4                             NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>is_read</th>\n      <th>rating</th>\n      <th>review_text_incomplete</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1384</td>\n      <td>1bad0122cebb4aa9213f9fe1aa281f66</td>\n      <td>True</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>Wed May 09 09:33:44 -0700 2007</td>\n      <td>Wed May 09 09:33:44 -0700 2007</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1376</td>\n      <td>eb6e502d0c04d57b43a5a02c21b64ab4</td>\n      <td>True</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>Wed May 09 09:33:18 -0700 2007</td>\n      <td>Wed May 09 09:33:18 -0700 2007</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>30119</td>\n      <td>787564bef16cb1f43e0f641ab59d25b7</td>\n      <td>True</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>Sat Jan 13 13:44:20 -0800 2007</td>\n      <td>Wed Mar 22 11:45:08 -0700 2017</td>\n      <td>Tue Mar 01 00:00:00 -0800 1983</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>72fb0d0087d28c832f15776b0d936598</td>\n      <td>24769928</td>\n      <td>8c80ee74743d4b3b123dd1a2e0c0bcac</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Wed Apr 27 11:05:51 -0700 2016</td>\n      <td>Wed Apr 27 11:05:52 -0700 2016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>72fb0d0087d28c832f15776b0d936598</td>\n      <td>30119</td>\n      <td>2a83589fb597309934ec9b1db5876aaf</td>\n      <td>True</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>Mon Jun 04 18:58:08 -0700 2012</td>\n      <td>Mon Jun 04 18:58:13 -0700 2012</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         Unnamed: 0       book_id        rating\ncount  2.734350e+06  2.734350e+06  2.734350e+06\nmean   1.367174e+06  6.808744e+06  1.824787e+00\nstd    7.893390e+05  9.698381e+06  2.123223e+00\nmin    0.000000e+00  2.340000e+02  0.000000e+00\n25%    6.835872e+05  4.204000e+04  0.000000e+00\n50%    1.367174e+06  5.922210e+05  0.000000e+00\n75%    2.050762e+06  1.219330e+07  4.000000e+00\nmax    2.734349e+06  3.648548e+07  5.000000e+00","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.734350e+06</td>\n      <td>2.734350e+06</td>\n      <td>2.734350e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.367174e+06</td>\n      <td>6.808744e+06</td>\n      <td>1.824787e+00</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>7.893390e+05</td>\n      <td>9.698381e+06</td>\n      <td>2.123223e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>2.340000e+02</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>6.835872e+05</td>\n      <td>4.204000e+04</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.367174e+06</td>\n      <td>5.922210e+05</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.050762e+06</td>\n      <td>1.219330e+07</td>\n      <td>4.000000e+00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.734349e+06</td>\n      <td>3.648548e+07</td>\n      <td>5.000000e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions_df.tail()","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"                                  user_id   book_id  \\\n2734345  594c86711bd7acdaf655d102df52a9cb  10810038   \n2734346  594c86711bd7acdaf655d102df52a9cb   4488657   \n2734347  594c86711bd7acdaf655d102df52a9cb   5865674   \n2734348  594c86711bd7acdaf655d102df52a9cb  16170625   \n2734349  594c86711bd7acdaf655d102df52a9cb  16101638   \n\n                                review_id  is_read  rating  \\\n2734345  1b6982ab9342de8d0c560f74598b7b95    False       0   \n2734346  6fccf41aad2ef9d4392b2dc0c14cf77c    False       0   \n2734347  dea3556a048fc6b75f361baeaa0c49e9    False       0   \n2734348  a3d153de0b86bdf9eeef0d756f471649     True       5   \n2734349  ca49618c80bf2111ce717d1d795a74d9     True       4   \n\n        review_text_incomplete                      date_added  \\\n2734345                    NaN  Thu May 23 12:54:29 -0700 2013   \n2734346                    NaN  Sun Apr 28 09:26:58 -0700 2013   \n2734347                    NaN  Sun Apr 28 09:25:28 -0700 2013   \n2734348                    NaN  Sun Apr 21 16:29:46 -0700 2013   \n2734349                    NaN  Sat Apr 20 15:29:12 -0700 2013   \n\n                           date_updated read_at started_at  \n2734345  Thu May 23 12:54:29 -0700 2013     NaN        NaN  \n2734346  Sun Apr 28 09:27:11 -0700 2013     NaN        NaN  \n2734347  Sun Apr 28 09:25:28 -0700 2013     NaN        NaN  \n2734348  Fri May 31 10:29:35 -0700 2013     NaN        NaN  \n2734349  Sun Apr 21 14:21:37 -0700 2013     NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>is_read</th>\n      <th>rating</th>\n      <th>review_text_incomplete</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2734345</th>\n      <td>594c86711bd7acdaf655d102df52a9cb</td>\n      <td>10810038</td>\n      <td>1b6982ab9342de8d0c560f74598b7b95</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Thu May 23 12:54:29 -0700 2013</td>\n      <td>Thu May 23 12:54:29 -0700 2013</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2734346</th>\n      <td>594c86711bd7acdaf655d102df52a9cb</td>\n      <td>4488657</td>\n      <td>6fccf41aad2ef9d4392b2dc0c14cf77c</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Sun Apr 28 09:26:58 -0700 2013</td>\n      <td>Sun Apr 28 09:27:11 -0700 2013</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2734347</th>\n      <td>594c86711bd7acdaf655d102df52a9cb</td>\n      <td>5865674</td>\n      <td>dea3556a048fc6b75f361baeaa0c49e9</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Sun Apr 28 09:25:28 -0700 2013</td>\n      <td>Sun Apr 28 09:25:28 -0700 2013</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2734348</th>\n      <td>594c86711bd7acdaf655d102df52a9cb</td>\n      <td>16170625</td>\n      <td>a3d153de0b86bdf9eeef0d756f471649</td>\n      <td>True</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>Sun Apr 21 16:29:46 -0700 2013</td>\n      <td>Fri May 31 10:29:35 -0700 2013</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2734349</th>\n      <td>594c86711bd7acdaf655d102df52a9cb</td>\n      <td>16101638</td>\n      <td>ca49618c80bf2111ce717d1d795a74d9</td>\n      <td>True</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>Sat Apr 20 15:29:12 -0700 2013</td>\n      <td>Sun Apr 21 14:21:37 -0700 2013</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions_df.head()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"                            user_id   book_id  \\\n0  8842281e1d1347389f2ab93d60773d4d      1384   \n1  8842281e1d1347389f2ab93d60773d4d      1376   \n2  8842281e1d1347389f2ab93d60773d4d     30119   \n3  72fb0d0087d28c832f15776b0d936598  24769928   \n4  72fb0d0087d28c832f15776b0d936598     30119   \n\n                          review_id  is_read  rating review_text_incomplete  \\\n0  1bad0122cebb4aa9213f9fe1aa281f66     True       4                    NaN   \n1  eb6e502d0c04d57b43a5a02c21b64ab4     True       4                    NaN   \n2  787564bef16cb1f43e0f641ab59d25b7     True       5                    NaN   \n3  8c80ee74743d4b3b123dd1a2e0c0bcac    False       0                    NaN   \n4  2a83589fb597309934ec9b1db5876aaf     True       3                    NaN   \n\n                       date_added                    date_updated  \\\n0  Wed May 09 09:33:44 -0700 2007  Wed May 09 09:33:44 -0700 2007   \n1  Wed May 09 09:33:18 -0700 2007  Wed May 09 09:33:18 -0700 2007   \n2  Sat Jan 13 13:44:20 -0800 2007  Wed Mar 22 11:45:08 -0700 2017   \n3  Wed Apr 27 11:05:51 -0700 2016  Wed Apr 27 11:05:52 -0700 2016   \n4  Mon Jun 04 18:58:08 -0700 2012  Mon Jun 04 18:58:13 -0700 2012   \n\n                          read_at started_at  \n0                             NaN        NaN  \n1                             NaN        NaN  \n2  Tue Mar 01 00:00:00 -0800 1983        NaN  \n3                             NaN        NaN  \n4                             NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>is_read</th>\n      <th>rating</th>\n      <th>review_text_incomplete</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1384</td>\n      <td>1bad0122cebb4aa9213f9fe1aa281f66</td>\n      <td>True</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>Wed May 09 09:33:44 -0700 2007</td>\n      <td>Wed May 09 09:33:44 -0700 2007</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1376</td>\n      <td>eb6e502d0c04d57b43a5a02c21b64ab4</td>\n      <td>True</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>Wed May 09 09:33:18 -0700 2007</td>\n      <td>Wed May 09 09:33:18 -0700 2007</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>30119</td>\n      <td>787564bef16cb1f43e0f641ab59d25b7</td>\n      <td>True</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>Sat Jan 13 13:44:20 -0800 2007</td>\n      <td>Wed Mar 22 11:45:08 -0700 2017</td>\n      <td>Tue Mar 01 00:00:00 -0800 1983</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>72fb0d0087d28c832f15776b0d936598</td>\n      <td>24769928</td>\n      <td>8c80ee74743d4b3b123dd1a2e0c0bcac</td>\n      <td>False</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>Wed Apr 27 11:05:51 -0700 2016</td>\n      <td>Wed Apr 27 11:05:52 -0700 2016</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72fb0d0087d28c832f15776b0d936598</td>\n      <td>30119</td>\n      <td>2a83589fb597309934ec9b1db5876aaf</td>\n      <td>True</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>Mon Jun 04 18:58:08 -0700 2012</td>\n      <td>Mon Jun 04 18:58:13 -0700 2012</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# In this cell, I'll clean all the data, and make sure there aren't any anomalies\n\n\n# interactions_df.drop(['Unnamed: 0'], axis = 1, inplace = True)\ninteractions_df = interactions_df.drop(columns = ['review_id','is_read','review_text_incomplete','date_added','date_updated','read_at','started_at'])\ndisplay(interactions_df.head(5))\n\n","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"                            user_id   book_id  rating\n0  8842281e1d1347389f2ab93d60773d4d      1384       4\n1  8842281e1d1347389f2ab93d60773d4d      1376       4\n2  8842281e1d1347389f2ab93d60773d4d     30119       5\n3  72fb0d0087d28c832f15776b0d936598  24769928       0\n4  72fb0d0087d28c832f15776b0d936598     30119       3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1384</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1376</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>30119</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>72fb0d0087d28c832f15776b0d936598</td>\n      <td>24769928</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72fb0d0087d28c832f15776b0d936598</td>\n      <td>30119</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I still have to remove all the reviews with 0 ratings. Becuase, they haven't actually read the book\ninteractions_df = interactions_df[interactions_df.rating != 0]\ndisplay(interactions_df.head(5))","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"                            user_id  book_id  rating\n0  8842281e1d1347389f2ab93d60773d4d     1384       4\n1  8842281e1d1347389f2ab93d60773d4d     1376       4\n2  8842281e1d1347389f2ab93d60773d4d    30119       5\n4  72fb0d0087d28c832f15776b0d936598    30119       3\n5  ab2923b738ea3082f5f3efcbbfacb218   240007       4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1384</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>1376</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8842281e1d1347389f2ab93d60773d4d</td>\n      <td>30119</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72fb0d0087d28c832f15776b0d936598</td>\n      <td>30119</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>ab2923b738ea3082f5f3efcbbfacb218</td>\n      <td>240007</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions_df['user_idInt']= interactions_df['user_id'].str.replace('\\D+','').astype(float)\ninteractions_df = interactions_df.drop(columns = ['user_id'])\ndisplay(interactions_df)","execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"          book_id  rating    user_idInt\n0            1384       4  8.842281e+23\n1            1376       4  8.842281e+23\n2           30119       5  8.842281e+23\n4           30119       3  7.200087e+23\n5          240007       4  2.923738e+15\n...           ...     ...           ...\n2734340      2547       5  1.484971e+21\n2734341  11047097       4  6.951628e+21\n2734343   7433930       5  5.948671e+17\n2734348  16170625       5  5.948671e+17\n2734349  16101638       4  5.948671e+17\n\n[1229059 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>book_id</th>\n      <th>rating</th>\n      <th>user_idInt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1384</td>\n      <td>4</td>\n      <td>8.842281e+23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1376</td>\n      <td>4</td>\n      <td>8.842281e+23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>30119</td>\n      <td>5</td>\n      <td>8.842281e+23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30119</td>\n      <td>3</td>\n      <td>7.200087e+23</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>240007</td>\n      <td>4</td>\n      <td>2.923738e+15</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2734340</th>\n      <td>2547</td>\n      <td>5</td>\n      <td>1.484971e+21</td>\n    </tr>\n    <tr>\n      <th>2734341</th>\n      <td>11047097</td>\n      <td>4</td>\n      <td>6.951628e+21</td>\n    </tr>\n    <tr>\n      <th>2734343</th>\n      <td>7433930</td>\n      <td>5</td>\n      <td>5.948671e+17</td>\n    </tr>\n    <tr>\n      <th>2734348</th>\n      <td>16170625</td>\n      <td>5</td>\n      <td>5.948671e+17</td>\n    </tr>\n    <tr>\n      <th>2734349</th>\n      <td>16101638</td>\n      <td>4</td>\n      <td>5.948671e+17</td>\n    </tr>\n  </tbody>\n</table>\n<p>1229059 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Supporting Functions\nHere I'll try to convert the entire data in form of scipy.sparse matrices, and download that data using scipy.sparse.save_npz(). This way, perhaps, our memory usage would be a bit lesser, and we could fit more of the data in the 16 Gigs we have at our disposal."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef showImagesHorizontally(list_of_files):\n    '''\n    This function shows all the books in a horizontal row within the ipython notebook\n    '''\n    fig = figure()\n    number_of_files = len(list_of_files)\n    for i in range(number_of_files):\n        a=fig.add_subplot(1,number_of_files,i+1)\n        image = imread(list_of_files[i])\n        imshow(image)\n        axis('off')\n\n        \ndef print_book_images(book_df, recommendations):\n    '''\n    This function will display the images along with Titles and author names of all the books recommended \n    Args:\n        recommendations (iterable) -> Containing the book_ids of the recommendations made by the recommender system\n        book_df (DataFrame) -> DataFrame containing basic info of all the books part of our product ('book_id','url','image_url','authors', 'title')\n        \n        \n    '''\n    #import requests\n    #import IPython.display as Disp\n    #url = 'https://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg'\n    #Disp.Image(requests.get(url).content)\n    temp_df = book_df.loc[book_df['book_id'] == recommendations]\n    width_pixels = 200\n    height_pixels = 300\n    book_urls = []\n    for id in recommendations:\n        book = book_df.loc[book_df['book_id'] == id]\n        image_url = book['image_url']\n        book_urls.append(image_url)\n        title = book['title']\n        authors = book['authors']\n        Image(image_url,width=width_pixels, height=height_pixels)\n        print('Title : ', title)\n        print('Authors : ', authors)\n        width_pixels = width_pixel - 10\n        height_pixels = height_pixels - 8\n    \n    showImagesHorizontally(book_urls)\n    \n    \n    \ndef print_book_ratings(data):\n    '''\n    This function will print the book ratings of the top-k book recommendations passed along to it\n    \n    '''\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Functions for Downloading the sparce matrices"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_sparse_matrix(data, user_dict, item_dict):\n    '''\n    This function creates a csr sparse matrix from the given data. Data should be a Dataframe (preferably derived from a .csv file)\n    \n    Args:\n        data (DataFrame) -> Containing book_id, user_idInt, and rating for each instance\n        user_dict(Pandas series) -> Contains the unique set of user ids present within the data\n        item_dict (Pandas Series) -> Contains the unique set of items present in the data\n    Returns:\n        csr_item_matrix (sparse csr matrix) -> csr item matrix created from the data\n        user_dict, item_dict -> Both updated\n    '''\n    # Here I'll create 'user_dict' & 'item_dict' such that: the indexes of the array would represent the \"key\" for the dicts.\n    temp_user_dict = pd.concat([user_dict, pd.unique(data['user_idInt'])], ignore_index = True)\n    temp_item_dict = pd.append(pd.unique(data['book_id']), ignore_index = True)\n    user_dict = pd.unique(temp_user_dict)\n    item_dict = pd.unique(temp_item_dict)\n\n    # Then, implement the csr_mechanism and save the result in self.csr_item_matrix\n    rows = []\n    cols = []\n    data = []\n    for i in range(len(data)):\n        rows.append(user_dict.index(data.loc[i, 'user_idInt'])) # Here we creating the row values for csr matrix using the index values of the user_dict we created by mapping unique values in a dataframe\n        cols.append(item_dict.index(data.loc[i, 'book_id'])) # Maybe, the index part should be in square brackets as well... Will see\n        data.append(data.loc[i, 'rating'])\n    rows = np.array(rows)\n    cols = np.array(cols)\n    data = np.array(data)\n    csr_item_matrix = csr_matrix(data, (row, col))\n    return csr_item_matrix.tocsr(), user_dict, item_dict\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here I'll create the sparse matrix using create_sparse_matrix function and then download it. Following which I'll upload it directly, and see if I can save further memory space this way\ndef download_sparse_matrix(filename, csr_matrix):\n    '''\n    This function will download the sparse matrix. I will be calling this function multiple times to convert and store all the interactions data in this very format.\n    \n    Actually, I might not have to make a seperate npz file for each genre data, instead, I can create a single npz file with multiple (compressed) chunks of data from \n    all the genres. The good thing about npz format is that it allows for multiple files to be stored such that only the ones being used are loaded onto the memory.\n    Checkout -https://stackoverflow.com/questions/54238670/what-is-the-advantage-of-saving-npz-files-instead-of-npy-in-python-regard\n    \n    \n    '''\n    sp.save_npz(filename, matrix)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Implementation code for the supporting functions\n\nHere, we'll be calling all the supporting functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here, I will clean the interactions data (removing unnecessary columns: maybe this can be done in a function, so that different data can be produced for implicit and explicit models) & call the two functions above. \n\n# Create_sparse_Matrix will probably have to be called for each genre or chunk we want, while (hopefully) download sparse can be called once.\n#  I will also have to look into how to add files to .npz format. Cos, maybe I can't load all the json data and convert it into npy files in one go.\n\n\n# Create user_dict and item_dict (to maintain uniqueness across varioud datasets)\nuser_dict = pd.DataFrame()\nitem_dict = pd.DataFrame()\nbook_df = pd.DataFrame(columns = ['book_id','url','image_url','authors', 'title'])\n# Perform the following code for each of the genre, and make sure user_dict and item_dict are maintained along all of them\n\n# Convert data and add it in form of lists\ninteractions_data = []\nwith open(\"/kaggle/input/goodreads-interactions-poetry/goodreads_interactions_poetry.json\", 'r') as f:\n    for line in f:\n        interaction_data.append(json.loads(line))\n\nbooks_data = []\nwith open(\"/kaggle/input/goodbooks-books-poetry/goodreads_books_poetry.json\", 'r') as f:\n    for line in f:\n        books_data.append(json.loads(line))\n\n# Convert it into a dataframe and store it according to genre name\ninteraction_df_poetry = pd.DataFrame(interaction_data)\nbook_df_poetry = pd.DataFrame(books_data)\n\n# Covert it into a csv file\n#interaction_df_poetry.to_csv('interactions_poetry.csv')\n\n# Drop the unnecessary columns and change the data type of the user_id column\ninteraction_df_poetry = interaction_df_poetry.drop(columns = ['review_id','is_read','review_text_incomplete','date_added','date_updated','read_at','started_at'])\nbook_df_poetry = book_df_poetry.drop(columns = ['work_id','isbn','series','asin','kindle_asin','description','link','isbn13','edition_information','title_without_series','country_code','text_reviews_count','language_code','publication_day','publication_month','is_ebook','format','publication_year','ratings_count', 'popular_shelves','publisher','similar_books'])\ninteraction_df_poetry['user_idInt']= interaction_df_poetry['user_id'].str.replace('\\D+','').astype(float)\n\n# Create and download the csr matrices\npoetry_csr_matrix, user_dict, item_dict = create_sparse_matrix(interaction_df_poetry, user_dict, item_dict)\ndownload_sparse_matrix('interactions_poetry', poetry_csr_matrix)\n\n# Create a unified book_df, to store: book_id, url, image_url, authors, and title of each book. Make sure that the book_df variable doesn't go away on each iteration of coding\nbook_df = pd.concat([book_df, book_df_poetry], ignore_index = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This part is intended to convert the json files of various genres (the original data form), into .csv files.\n# I have to put it in a function, so that it can be used effectively.\n'''interaction_data = []\nwith open(\"/kaggle/input/goodreads-interactions-poetry/goodreads_interactions_poetry.json\", 'r') as f:\n    for line in f:\n        interaction_data.append(json.loads(line))\n        \n#interaction_data1 = interaction_data[:1000000]\ninteraction_df_poetry = pd.DataFrame(interaction_data)\ninteraction_df_poetry.head(10)\ndisplay(interaction_df_poetry.describe())\ninteraction_df_poetry.to_csv('interactions_poetry.csv')\nprint(getsizeof(interaction_data1))\nprint(\"Dataframe size is\", getsizeof(interaction_df1))\noriginal_df = pd.DataFrame(columns = ['user_id', 'book_id', 'review_id', 'is_read', 'rating', 'review_text_incomplete', 'date_added', 'date_updated', 'read_at', 'started_at'])\ntemp_df = pd.DataFrame(columns = ['user_id', 'book_id', 'review_id', 'is_read', 'rating', 'review_text_incomplete', 'date_added', 'date_updated', 'read_at', 'started_at'])\nwith open(\"/kaggle/input/goodreads-interactions-poetry/goodreads_interactions_poetry.json\", 'r') as f:\n    for line in f:\n        data = json.loads(line)\n        original_df = pd.concat([original_df, pd.json_normalize(data)])\n        #original_df.concat(json_normalize(data, '0'))\ndisplay(interaction_df1.head())\ndisplay(interaction_df1.info())\n'''\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualizations"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 1: Neighborhood Modelling\nI will try to do this part in the following two steps:\n1. I will create the sparse matrix of the data and use them on distance metric code from scratch\n2. Use the same sparse matrix to then evaluate the results using the implementation of similaripy"},{"metadata":{},"cell_type":"markdown","source":"### 1. Using Sparse Matrices and simple Cosine Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"class knn_original():\n    def __init__(self, k = 20, data = None, distance_metric = 'dot_product', is_already_sparse = False, csr_item_matrix = None, alpha = 0.6):\n        '''\n        This class produces recommendations based on item-item similarity measures.\n        The distance metrics used in this class are written from scratch, and don't have the speed benefits of the cython used by the library implementation following this.\n        \n        Args:\n            data (DataFrame) -> Containing user_item interactions and their ratings.\n            distance_metric (object) -> Defines the distance metric to be used\n            k (integer) -> Represents the number of similar items that we have to produce\n            csr_item_matrix (sparse matrix) -> csr matrix with items as rows and users as columns (optional)\n            is_already_sparse (boolean) -> Defines if the data is already sparse.\n            \n        Returns:\n            similarity_matrix (sparse_matrix) -> Contains the top-k similar items for each item of the data entered.\n            \n        '''\n        _DISTANCE = ('dot_product', 'cosine','asymmetric_cosine')\n        assert(distance_metric in _DISTANCE), 'The distance_metric has to be one of the following: dot_product, cosine, asymmetric_cosine' \n        self.k = k\n        self.data = data\n        self.csr_item_matrix = csr_item_matrix\n        if is_already_sparse == False:\n            self.create_csr_data()\n        self.distance_metric = distance_metric\n        self.alpha = alpha\n        self.similarity_item_matrix = self.fit()\n    \n    \n    \n    def create_csr_data(self):\n        '''\n        This converts the data into the sparse matrix fromat. Again, we wouldn't need this if we end up using the supporting functions!\n        '''\n        # For now, I will have to clean the data first. That is, convert the columns into desired data type and remove the unecessary ones.\n        self.data = self.data.drop(columns = ['review_id','is_read','review_text_incomplete','date_added','date_updated','read_at','started_at'])\n        self.data['user_idInt']= self.data['user_id'].str.replace('\\D+','').astype(float)\n        \n        # Here I'll create 'user_dict' & 'item_dict' such that: the indexes of the array would represent the \"key\" for the dicts.\n        self.user_dict = pd.unique(self.data['user_idInt'])\n        self.item_dict = pd.unique(self.data['book_id'])\n        \n        \n        # Then, implement the csr_mechanism and save the result in self.csr_item_matrix\n        rows = []\n        cols = []\n        data = []\n        for i in range(len(self.data)):\n            rows.append(self.user_dict.index(self.data.loc[i, 'user_idInt'])) # Here we creating the row values for csr matrix using the index values of the user_dict we created by mapping unique values in a dataframe\n            cols.append(self.item_dict.index(self.data.loc[i, 'book_id'])) # Maybe, the index part should be in square brackets as well... Will see\n            data.append(self.data.loc[i, 'rating'])\n        rows = np.array(rows)\n        cols = np.array(cols)\n        data = np.array(data)\n        self.csr_item_matrix = csr_matrix(data, (row, col))\n    \n        \n    \n    def dot_product(self, matrix):\n        '''\n        Finds the dot product similarity measure of the data\n        \n        '''\n        s = matrix * matrix.T\n        return self.top_k_items(s, self.k)\n\n        \n    def cosine(self, matrix):\n        '''\n        Finds the cosine similarity measure of the data\n        \n        '''\n        matrix2 = matrix.copy()\n        matrix2.data = np.power(matrix2.data,2)\n        X = np.power(matrix2.sum(axis=1).A1,0.5)\n        matrix_aux = (matrix * matrix.T).tocsr()\n        r, c, v = [], [], []\n        for idx1 in range(0,matrix.shape[0]):\n            for idx2 in range(matrix_aux.indptr[idx1], matrix_aux.indptr[idx1+1]):\n                row = idx1\n                col = matrix_aux.indices[idx2]\n                val = matrix_aux.data[idx2]\n                r.append(row)\n                c.append(col)\n                v.append(val/ (X[row] * X[col]))\n        s = sp.csr_matrix((v,(r,c)),shape=(matrix.shape[0],matrix.shape[0]))\n        return self.top_k_items(s, self.k)\n\n    \n    def asymmetric_cosine(matrix, alpha):\n        '''\n        Finds the assymetrical cosine similarity measure\n        \n        '''\n        matrix2 = matrix.copy()\n        matrix2.data = np.power(matrix2.data,2)\n        X = np.power(matrix2.sum(axis=1).A1,alpha)\n        Y = np.power(matrix2.sum(axis=1).A1,1-alpha)\n        matrix_aux = (matrix * matrix.T).tocsr()\n        r, c, v = [], [], []\n        for idx1 in range(0,matrix.shape[0]):\n            for idx2 in range(matrix_aux.indptr[idx1], matrix_aux.indptr[idx1+1]):\n                row = idx1\n                col = matrix_aux.indices[idx2]\n                val = matrix_aux.data[idx2]\n                r.append(row)\n                c.append(col)\n                v.append(val/ (X[row] * Y[col]))\n        s = sp.csr_matrix((v,(r,c)),shape=(matrix.shape[0],matrix.shape[0]))\n        return self.top_k_items(s, self.k)\n        \n        \n    def top_k_items(self, X, k):\n        '''\n        \n        '''\n        X = X.tocsr()\n        r, c, d = [], [], []\n        for i in range(X.shape[0]):\n            data = X.data[X.indptr[i]:X.indptr[i+1]]\n            topk = min(k, data.shape[0])\n            indices = X.indices[X.indptr[i]:X.indptr[i+1]]\n            topk_idx = np.argpartition(data, -topk)[-topk:]\n            data = data[topk_idx]\n            indices = indices[topk_idx]\n            r += np.full(topk, i).tolist()\n            c += indices.tolist()\n            d += data.tolist()\n        return sp.csr_matrix((d, (r, c)), shape=X.shape)\n        \n        \n        \n    def fit_model(self):\n        '''\n        The central function that calls the necesary functions to produce top-k recommendations\n        \n        '''\n        self.old_csr_item_matrix = self.csr_item_matrix.copy()\n        if self.distance_metric == 'dot_product':\n            similarity_scores = self.dot_product(self.csr_item_matrix)\n        elif self.distance_metric == 'cosine':\n            similarity_scores = self.cosine(self.csr_item_matrix)\n        else:\n            similarity_scores = self.asymmetric_cosine(self.csr_item_matrix, self.alpha)\n        return similarity_scores\n        \n    \n    def make_recommendations(self,user_id):\n        '''\n        \n        '''\n        self.user_id = user_id\n        self.user_id = self.user_id.str.replace('\\D+','').astype(float)\n        user_id_to_check = self.user_dict.index(self.user_id)\n        matrix = self.old_csr_item_matrix.T.tocsr()\n        indices = matrix.indices\n        indptr = matrix.indptr\n        data = matrix.data\n        book_list = []\n        for i in range(indptr[user_id_to_check], indptr[user_id_to_check + 1]):\n            book_list.append(data[i].astype(int))\n        indices = self.similarity_scores.indices\n        indptr = self.similairity_scores.indptr\n        data = self.similarity_scores.data\n        positional_increment = 0.05 # This defines the extra amount of importance added with better position in the list. So, if the last element (of a 10 item recommendation) is has a value of 1, the first element would have a value of 1 + (0.05 * 10)\n        recommendation_df = pd.DataFrame(columns = ['book_id', 'positional_value'])\n        for book in book_list:\n            top_k_recommendations = []\n            for i in range(indptr[book], indptr[book + 1]):\n                top_k_recommendations.append(data[i].astype(int))\n            start_value = 1\n            m = len(top_k_recommendations)\n            for i in range(m - 1, -1, -1):\n                book_id = top_k_recommendations[i]\n                position_value = start_value + (m - (i+1))*positional_increment\n                book_df = pd.DataFrame([book_id, positional_value], columns = ['book_id','positional_value'])\n                recommendation_df = pd.concat([recommendation_df, book_df], axis = 1)\n        recommendation_df = recommendation_df.groupby('book_id').sum()\n        recommendation_df = recommendation_df.sort_values('positional_value')[:self.k]\n        self.recommendations = np.array(recommendation_df.loc[:,'book_id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Using Sparse Matrices and Similaripy Implementation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# I can even include the train test split and evaluate the model's efficacy using RMSE, but for that, I will have to figure out how to extract similarity measures from \n# the implicit library, and then transform those similarity measures using \nclass knn_similaripy():\n    def __init__(self, normalization_type = None, tf_type= 'sqrt', idf_type = 'bm25', distance_metric = 'cosine',\n                 user_id = None, data = None, csr_item_matrix = None, k = 10, is_already_sparse = False,\n                 create_ratings = False):\n        '''\n        This class produces recommendations based on Nearest Neighbour mechanism. We will use the KNN implementation of implicit library to perform this task.\n        \n        Args:\n            data (DataFrame) -> Actual DataFrame containing three columns - 'book_id', 'user_id', 'rating'\n            user_id (Int) -> User Id of the user for which we want to make the recommendations\n            k (int) -> Number of recommendations we want to produce\n        \n        Returns:\n            recommendations (iterable) -> A list/array of item_id, of the books being recommended.\n        \n        '''\n        _NORMALIZATIONS = ('l1', 'l2', 'max')\n        _TF = ('binary', 'raw', 'sqrt', 'freq', 'log')\n        _IDF = ('unary', 'base', 'smooth', 'prob', 'bm25', 'bm25plus')\n        _DISTANCE_METRIC = ('dot_product', 'cosine', 'assymetric_cosine','jaccard','dice','tversky','p3alpha','rp3beta')\n        assert(normalization_type in _NORMALIZATIONS), 'Normalization functions can only be l1, l2, max'\n        assert(tf_type in _TF), 'Term Frequency functions can only be binary, raw, sqrt, freq, log'\n        assert(idf_type in _IDF), 'Inverse Document Frequency functions can only be one of unary, base, smooth, prob, bm25, bm25plus'\n        assert(distance_metric in _DISTANCE_METRIC), 'Distance Metric must be one of the many prescribed by Similaripy'\n        self.user_id = user_id\n        self.data = data\n        self.k = k\n        self.csr_item_matrix = csr_item_matrix # This will house the sparse matrix of the original data (if passed data isn't csr_matrix already)\n        self.is_already_sparse = is_already_sparse\n        if not self.is_already_sparse and data != None:\n            self.create_csr_data()\n        self.normalization_type = normalization_type\n        self.tf_type = tf_type\n        self.idf_type = idf_type\n        self.distance_metric = distance_metric\n        self.recommendations = None\n        self.ratings_matrix = None # Here I will store the predicted ratings for all user item pairs. Special care will have to be taken to ensure that this does not excede memory.\n        self.create_ratings = create_ratings\n        self.train_data = None # Here I will store the train and test data, respectively\n        self.test_data = None\n        if self.create_ratings:\n            self.test_train_split()\n        self.similarity_scores = self.fit_model()\n        \n        \n        \n    def create_csr_data(self):\n        '''\n        This function converts the original dataframe into a sparse matrix.\n        \n        '''\n        # For now, I will have to clean the data first. That is, convert the columns into desired data type and remove the unecessary ones.\n        self.data = self.data.drop(columns = ['review_id','is_read','review_text_incomplete','date_added','date_updated','read_at','started_at'])\n        self.data['user_idInt']= self.data['user_id'].str.replace('\\D+','').astype(float)\n        \n        # Here I'll create 'user_dict' & 'item_dict' such that: the indexes of the array would represent the \"key\" for the dicts.\n        self.user_dict = pd.unique(self.data['user_idInt'])\n        self.item_dict = pd.unique(self.data['book_id'])\n        \n        \n        # Then, implement the csr_mechanism and save the result in self.csr_item_matrix\n        rows = []\n        cols = []\n        data = []\n        for i in range(len(self.data)):\n            rows.append(self.user_dict.index(self.data.loc[i, 'user_idInt'])) # Here we creating the row values for csr matrix using the index values of the user_dict we created by mapping unique values in a dataframe\n            cols.append(self.item_dict.index(self.data.loc[i, 'book_id'])) # Maybe, the index part should be in square brackets as well... Will see\n            data.append(self.data.loc[i, 'rating'])\n        rows = np.array(rows)\n        cols = np.array(cols)\n        data = np.array(data)\n        self.csr_item_matrix = csr_matrix(data, (row, col))\n    \n    \n    def fit_model(self):\n        '''\n        This function fits the Similaripy model to the given data, and creates the 'similarity_matrix' for all the items.\n        \n        First I'll normalize the data and tranform with the appropriate tf_idf transform.\n        Then, I'll feed the data with the specific distance metric and produce the similarity matrix\n        \n        \n        '''\n        self.old_csr_item_matrix = self.csr_item_matrix.copy()\n        # Normalization part\n        if self.normalization_type == None:\n            continue\n        else:\n            self.csr_item_matrix = normalize(self.csr_item_matrix, axis = 1, norm = self.normalization_type)\n            \n        # Tf_IDf part\n        if self.tf_type == None and self.idf_type == None:\n            continue\n        elif self.idf_type == 'bm25':\n            self.csr_item_matrix = bm25(self.csr_item_matrix, tf_mode = self.tf_type, idf_mode = self.idf_type)\n        elif self.idf_type == 'bm25plus':\n            self.csr_item_matrix = bm25plus(self.csr_item_matrix, tf_mode = self.tf_type, idf_mode = self.idf_type)\n        else:\n            self.csr_item_matrix = tfidf(self.csr_item_matrix, tf_mode = self.tf_type, idf_mode = self.idf_type)\n        \n        # Fitting the model (!!!Still have to figure out if this returns the a csr matrix with top k items for each item or not!!!)\n        if self.distance_metric == 'dot_product':\n            model = sim.dot_product(self.csr_item_matrix, k = self.k)\n        elif self.distance_metric == 'cosine':\n            model = sim.cosine(self.csr_item_matrix, k = self.k)\n        elif self.distance_metric == 'assymetric_cosine':\n            model = sim.assymetric_cosine(self.csr_item_matrix, alpha = 0.2, k = self.k)\n        elif self.distance_metric == 'jaccard':\n            model = sim.jaccard(self.csr_item_matrix, k = self.k)\n        elif self.distance_metric == 'dice':\n            model = sim.dice(self.csr_item_matrix, k = self.k)\n        elif self.distance_metric == 'tversky':\n            model = sim.tversky(self.csr_item_matrix, alpha = 0.8, beta = 0.4, k = self.k)\n        elif self.distance_metric == 'p3alpha':\n            model = sim.p3alpha(self.csr_item_matrix, alpha = 0.8, k = self.k)\n        else:\n            model = sim.rp3beta(self.csr_item_matrix, alpha = 0.8, beta = 0.4, k = self.k)\n        \n        return model # Here I am assuming that the model creates similarity scores of top-k items for each item in self.similarity_scores (which it most probably does!)\n    \n    \n    def create_recommendations(self, user_id = 1, positional_increment = 0.05):\n        '''\n        This function covers the utility aspect of the recommendation systems, by making book recommendations for the given user_id, by making sure that the user history is \n        taken into account.\n        \n        Since, this model produces similarity scores of the items, producing recommendations for a specific user has to do the following task:\n        It produces top-K similar books for each of the items that the user has rated. It then produces a normalized score of those recommendations (based on user rating).\n        The top-K books thus produced are then recommended to the user.\n        \n        Here, I will have to decide if I want to implement that normalization approach where the baseline ratings are removed from the rating_score being generated,\n        to account for the user bias (some users are prone to giving too good or too bad ratings)\n        \n        Args:\n            user_id (object/int) -> User_id either in form given in the dataset or in integer form\n            positional_increment (float) -> Defines the extra bit of importance we give to having a single position advantage in recommendations\n        \n        '''\n        self.user_id = user_id\n        self.user_id = self.user_id.str.replace('\\D+','').astype(float)\n        user_id_to_check = self.user_dict.index(self.user_id)\n        matrix = self.old_csr_item_matrix.T.tocsr()\n        indices = matrix.indices\n        indptr = matrix.indptr\n        data = matrix.data\n        book_list = []\n        for i in range(indptr[user_id_to_check], indptr[user_id_to_check + 1]):\n            book_list.append(data[i].astype(int))\n        indices = self.similarity_scores.indices\n        indptr = self.similairity_scores.indptr\n        data = self.similarity_scores.data\n        positional_increment = 0.05 # This defines the extra amount of importance added with better position in the list. So, if the last element (of a 10 item recommendation) is has a value of 1, the first element would have a value of 1 + (0.05 * 10)\n        recommendation_df = pd.DataFrame(columns = ['book_id', 'positional_value'])\n        for book in book_list:\n            top_k_recommendations = []\n            for i in range(indptr[book], indptr[book + 1]):\n                top_k_recommendations.append(data[i].astype(int))\n            start_value = 1\n            m = len(top_k_recommendations)\n            for i in range(m - 1, -1, -1):\n                book_id = top_k_recommendations[i]\n                position_value = start_value + (m - (i+1))*positional_increment\n                book_df = pd.DataFrame([book_id, positional_value], columns = ['book_id','positional_value'])\n                recommendation_df = pd.concat([recommendation_df, book_df], axis = 1)\n        recommendation_df = recommendation_df.groupby('book_id').sum()\n        recommendation_df = recommendation_df.sort_values('positional_value')[:self.k]\n        self.recommendations = np.array(recommendation_df.loc[:,'book_id'])\n    \n    \n    def test_train_split(self):\n        '''\n        This function will only be used if we are gonna produce ratings as well, along with recommendations. The main problem with this approach would be that we can \n        only create ratings for the items that do fall into the 'top-K' similar items for each item, and there are high chances that the user might not have actually rated\n        any/all of the books that we are creating the ratings for. \n        So for a sufficiently large enough test set, the number actual test instances that we can compare our model's prediction capability would be actually quite small.\n        \n        '''\n        \n        \n        \n        \n        \n        \n    def create_ratings(self):\n        '''\n        This function will convert the similarity values, (derived from fit_model() function) using the approach defined in 'Neigborhood meets Factorization' paper\n        \n        \n        '''\n        \n        \n        \n        \n        \n        \n    def efficacy_score(self):\n        '''\n        Using the test train split, and after creating the ratings, this function will produce the error metric on recommendations using RMSE score as the deciding metric.\n        This would allow us to compare various models amongst each other\n        \n        '''\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"poetry_nn_df1 = interactions_df.copy()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Shortcomings still remaining:\n* Point 1\n* Point 2"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 2: Matrix Factorization Method"},{"metadata":{},"cell_type":"markdown","source":"### Surprise Implementation"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Creating Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the data into the format Surprise Requires it in\nreader = Reader(rating_scale=(1, 5))\ndata_surprise = Dataset.load_from_df(interactions_df[['user_idInt', 'book_id', 'rating']], reader)\n# Creating train and test splits, with 10 percent of data being saved for \ntraining_data, testing_data = train_test_split(data_surprise, test_size=.10)\n\n","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Defining Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"algo1 = SVD(n_factors = 20, n_epochs = 10, verbose = True)\nalgo1.fit(training_data)","execution_count":18,"outputs":[{"output_type":"stream","text":"Processing epoch 0\nProcessing epoch 1\nProcessing epoch 2\nProcessing epoch 3\nProcessing epoch 4\nProcessing epoch 5\nProcessing epoch 6\nProcessing epoch 7\nProcessing epoch 8\nProcessing epoch 9\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f15e95389d0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we try to increase the number of factors to 50, and number of epochs to 30\nalgo2 = SVD(n_factors = 50, n_epochs = 30, verbose = True)\nalgo2.fit(training_data)","execution_count":46,"outputs":[{"output_type":"stream","text":"Processing epoch 0\nProcessing epoch 1\nProcessing epoch 2\nProcessing epoch 3\nProcessing epoch 4\nProcessing epoch 5\nProcessing epoch 6\nProcessing epoch 7\nProcessing epoch 8\nProcessing epoch 9\nProcessing epoch 10\nProcessing epoch 11\nProcessing epoch 12\nProcessing epoch 13\nProcessing epoch 14\nProcessing epoch 15\nProcessing epoch 16\nProcessing epoch 17\nProcessing epoch 18\nProcessing epoch 19\nProcessing epoch 20\nProcessing epoch 21\nProcessing epoch 22\nProcessing epoch 23\nProcessing epoch 24\nProcessing epoch 25\nProcessing epoch 26\nProcessing epoch 27\nProcessing epoch 28\nProcessing epoch 29\n","name":"stdout"},{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f158428c750>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"algo3 = SVD(n_factors = 100, n_epochs = 30, lr_all = 0.0075, reg_all = 0.03, verbose = True)\nalgo3.fit(training_data)","execution_count":48,"outputs":[{"output_type":"stream","text":"Processing epoch 0\nProcessing epoch 1\nProcessing epoch 2\nProcessing epoch 3\nProcessing epoch 4\nProcessing epoch 5\nProcessing epoch 6\nProcessing epoch 7\nProcessing epoch 8\nProcessing epoch 9\nProcessing epoch 10\nProcessing epoch 11\nProcessing epoch 12\nProcessing epoch 13\nProcessing epoch 14\nProcessing epoch 15\nProcessing epoch 16\nProcessing epoch 17\nProcessing epoch 18\nProcessing epoch 19\nProcessing epoch 20\nProcessing epoch 21\nProcessing epoch 22\nProcessing epoch 23\nProcessing epoch 24\nProcessing epoch 25\nProcessing epoch 26\nProcessing epoch 27\nProcessing epoch 28\nProcessing epoch 29\n","name":"stdout"},{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7f158425fa90>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Evaluating RMSE scores of test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = algo1.test(testing_data)\nprint(predictions[0:5])\npredicted_values = [prediction[3] for prediction in predictions]\nactual_values = [prediction[2] for prediction in predictions]\nscore = mean_squared_error(predicted_values, actual_values)\nprint('Mean Squared Error for Algo1 is ', score)\n","execution_count":45,"outputs":[{"output_type":"stream","text":"[Prediction(uid=7.990865635606991e+18, iid=5365890, r_ui=5.0, est=4.21245686299496, details={'was_impossible': False}), Prediction(uid=3334632421182.0, iid=23513349, r_ui=5.0, est=4.039836109510277, details={'was_impossible': False}), Prediction(uid=5.117867507246729e+23, iid=135914, r_ui=2.0, est=4.434237960252834, details={'was_impossible': False}), Prediction(uid=3.394076697144432e+18, iid=1371, r_ui=5.0, est=3.5618849103250705, details={'was_impossible': False}), Prediction(uid=1.9102173198936143e+18, iid=5865595, r_ui=4.0, est=3.69674387896062, details={'was_impossible': False})]\nMean Squared Error is  0.7599449695931739\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = algo2.test(testing_data)\nprint(predictions[0:5])\npredicted_values = [prediction[3] for prediction in predictions]\nactual_values = [prediction[2] for prediction in predictions]\nscore = mean_squared_error(predicted_values, actual_values)\nprint('Mean Squared Error for Algo1 is ', score)","execution_count":47,"outputs":[{"output_type":"stream","text":"[Prediction(uid=7.990865635606991e+18, iid=5365890, r_ui=5.0, est=4.255335465276854, details={'was_impossible': False}), Prediction(uid=3334632421182.0, iid=23513349, r_ui=5.0, est=3.4619858989446866, details={'was_impossible': False}), Prediction(uid=5.117867507246729e+23, iid=135914, r_ui=2.0, est=4.344592732722931, details={'was_impossible': False}), Prediction(uid=3.394076697144432e+18, iid=1371, r_ui=5.0, est=2.9907983023522045, details={'was_impossible': False}), Prediction(uid=1.9102173198936143e+18, iid=5865595, r_ui=4.0, est=4.115873258407759, details={'was_impossible': False})]\nMean Squared Error for Algo1 is  0.7430516785096469\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = algo3.test(testing_data)\nprint(predictions[0:5])\npredicted_values = [prediction[3] for prediction in predictions]\nactual_values = [prediction[2] for prediction in predictions]\nscore = mean_squared_error(predicted_values, actual_values)\nprint('Mean Squared Error for Algo1 is ', score)","execution_count":49,"outputs":[{"output_type":"stream","text":"[Prediction(uid=7.990865635606991e+18, iid=5365890, r_ui=5.0, est=4.226096772677253, details={'was_impossible': False}), Prediction(uid=3334632421182.0, iid=23513349, r_ui=5.0, est=3.8330093413615964, details={'was_impossible': False}), Prediction(uid=5.117867507246729e+23, iid=135914, r_ui=2.0, est=4.3549111890341585, details={'was_impossible': False}), Prediction(uid=3.394076697144432e+18, iid=1371, r_ui=5.0, est=3.3684022550126596, details={'was_impossible': False}), Prediction(uid=1.9102173198936143e+18, iid=5865595, r_ui=4.0, est=3.907478088763133, details={'was_impossible': False})]\nMean Squared Error for Algo1 is  0.7451192479396285\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"##### Result Evaluation and comparison of various models\n* Increasing the number of factors and number of epochs helped improve the MSE score.\n* Increasing the learning rate and relaxing the regularization rate actually detoriated the performance of the algorithm a bit"},{"metadata":{},"cell_type":"markdown","source":"After training the model with the Surprise implementation of SVD, we could produce the recommendations using the following metric:\n* Simple rating values of all the items for a given user, and then print the top 10 from the sorted list\n* Rating values of only the top-k items recommended by the item similarity algorithm, and printing them out. This would allow us to compare the efficacy of different models."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Shortcomings still remaining:\n* It does not use sparse matrices. Therefore making it tough for working on the big files I have\n* While it has a 'dump' function for downloading the trained model and loading it later, it does not have iterative training. Therefore, it will have to train the model from scratch each time\n* It does not have an option for trying out ALS implementation of MF.\n* The SVD implementation of Surprise cannot accomodate implicit data, we will try to incorporate that in the next section using SVD++"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 3: Weighted Regularized Matrix Factorization"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Part 4: Neural Collaborative Filtering (Optional)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}