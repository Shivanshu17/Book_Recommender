{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Loading Process"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn import preprocessing\nimport gc\n\n'''interaction_data = []\nwith open(\"/kaggle/input/goodreads-interactions-poetry/goodreads_interactions_poetry.json\", 'r') as f:\n    for line in f:\n        interaction_data.append(json.loads(line))'''\n        \nreview_data = []\nwith open(\"/kaggle/input/goodreads-review-poetry/goodreads_reviews_poetry.json\", 'r') as f:\n    for line in f:\n        review_data.append(json.loads(line))\n        \nbooks_data = []\nwith open(\"/kaggle/input/goodbooks-books-poetry/goodreads_books_poetry.json\", 'r') as f:\n    for line in f:\n        books_data.append(json.loads(line))\n        \n        \n# interaction_data1 = interaction_data\nreview_data1 = review_data[:1000000]\nbook_data1 = books_data[:1000000]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting into DataFrames\n\n# interaction_df1 = pd.DataFrame(interaction_data1)\nreview_df1 = pd.DataFrame(review_data1)\nbook_df1 = pd.DataFrame(book_data1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Visualization/Cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"review_df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# interaction_df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"book_df1.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# interaction_df1.info()\n\nbook_df1.info()\n\nreview_df1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Column Names of Interaction Data: {0}\",format(list(interaction_df1.columns)))\nprint(\"Column Names of Review Data : {0}\", format(list(review_df1.columns)))\nprint(\"Column Names of Book Data: {0}\", format(list(book_df1.columns)))\n\n# To check if there are any null values in the columns\n\nprint(interaction_df1.isnull().values.any())\nprint(review_df1.isnull().values.any())\nprint(book_df1.isnull().values.any())\n\n\n# To gauge the unique number of users and books\n\nprint(len(interaction_df1.user_id.unique()))\nprint(len(interaction_df1.book_id.unique()))\nprint(len(interaction_df1.date_added.unique()))\n\nprint(len(review_df1.user_id.unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop out the irrelevent columns. I'll drop the dates, months and year, alongwith country_code column because they will serve the purpose in implicit recommendation process.\nbook_df1 = book_df1.drop(columns = ['isbn','series','asin','kindle_asin','description','link','isbn13','edition_information','url','image_url','title','title_without_series','country_code','text_reviews_count','language_code','publication_day','publication_month','is_ebook','format','publication_year'])\n# We drop the irrelevant columns from explicit feedback collaborative filtering process.\nreview_df1 = review_df1.drop(columns = ['review_id','review_text','n_votes','n_comments','date_added','date_updated','read_at','started_at'])\nbook_df1 = book_df1.drop(columns =['work_id'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"book_df1.book_id = book_df1.book_id.astype(str).astype(int)\nbook_df1.ratings_count = book_df1.ratings_count.astype(str).astype(int)\n# The code says out of memory (at the matrix formation stage) whithout this modification (it would be preferable if we didn't have to place this restriction on the number of ratings)\nfor i in range(len(book_df1)):\n    if book_df1.ratings_count[i] < 50:\n        book_df1.drop([i], inplace=True)\n\nreview_df1.book_id = review_df1.book_id.astype(str).astype(int)\ndf = pd.merge(review_df1, book_df1, on='book_id')\ndf.average_rating = df.average_rating.astype(str).astype(float)\ndf.ratings_count = df.ratings_count.astype(str).astype(int)\ndf.user_id = df.user_id.astype(str).astype(str)\ndf.book_id = df.book_id.astype(str).astype(int)\ndf['user_idInt']=df['user_id'].str.replace('\\D+','').astype(float)\nprint(len(df.user_idInt.unique()))\ndf.drop_duplicates(['user_idInt','book_id'])\nprint(len(df.user_idInt.unique()))\npopular_SHELVES = df.popular_shelves\nsimilar_BOOKS = df.similar_books\ndf = df.drop(columns = ['popular_shelves','similar_books','ratings_count', 'authors'])\ndf = df.drop(columns = ['average_rating','publisher','num_pages'])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train/Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_copy = df.copy()\ndf_train = df_copy.sample(frac=0.8, random_state=0)\ndf_test = df_copy.drop(df_train.index)\n\n'''print(len(df.user_id.unique()))\nusers = df['user_idInt'].unique()\nbooks = df['book_id'].unique()\n\ntest = pd.DataFrame(columns=df.columns)\ntrain = pd.DataFrame(columns=df.columns)\ntest_ratio = 0.2 #fraction of data to be used as test set.\n\nfor u in users:\n    temp = df[df['user_idInt'] == u]\n    n = len(temp)\n    test_size = int(test_ratio*n)\n    \n\n    # temp.drop('index', axis=1, inplace=True)\n    \n    dummy_test = temp.ix[n-1-test_size :]\n    dummy_train = temp.ix[: n-2-test_size]\n    \n    test = pd.concat([test, dummy_test])\n    train = pd.concat([train, dummy_train])\n\nprint(test.info())\nprint(test.describe())'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a books_matrix with the item-user rating values.\nbooks_matrix = df_train.pivot_table(index = 'user_idInt', columns = 'book_id', values = 'rating').fillna(0)\nbooks_matrix.shape\nbooks_matrix.head(10)\n\nmean_rows = []\nmean_rows = books_matrix.mean(axis = 1, skipna = True) \n\nimport math\nfor column in books_matrix.columns:\n\n    books_matrix[column] = books_matrix[column].replace(np.nan, mean_rows[column])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nfrom sklearn.decomposition import TruncatedSVD\n\nX = books_matrix.values.T\nX.shape\n#Fitting the Model\nSVD = TruncatedSVD(n_components=5, random_state=0)\nmatrix1 = SVD.fit_transform(X)\nmatrix1.shape \n\nX = books_matrix.values\nX.shape\n#Fitting the Model\nSVD = TruncatedSVD(n_components=5, random_state=0)\nmatrix = SVD.fit_transform(X)\nmatrix.shape \n\nsvd_out = np.dot(matrix, matrix1.T)\n\ndef rmse(true, pred):\n    # this will be used towards the end\n    x = []\n    for i in range(len(true)):\n        x.append(true[i] - pred[i])\n    return sum([xi*xi for xi in x])/len(x)\n\n\n\n''' itemField = formatizer['item']\nuserField = formatizer['user']\n\n\nusers = list(set(data.ix[:,userField]))\nitems = list(set(data.ix[:,itemField]))\nusers_index = {users[i]: i for i in range(len(users))}\n\n\nitemcols = list(X.columns)\nitems_index = {itemcols[i]: i for i in range(len(itemcols))}'''\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation Stage"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = [] #to store the predicted ratings\ntest_cols = [] # to store the ratings of the users that we will actually be testing against\nfor _,row in df_test.iterrows():\n    user = row['user_idInt']\n    book = row['book_id']\n    if user in books_matrix.index and book in books_matrix.columns:\n        u_index = books_matrix.index.index(user)\n        i_index = books_matrix.columns.index(book)\n        pred_rating = svdout[u_index, i_index]\n        test_cols.append(row['ratings'])\n    elif user in books_matrix.index and book not in books_matrix.columns:\n        u_index = books_matrix.index.index(user)\n        pred_rating = np.mean(svdout[u_index, :])\n        test_cols.append(row['ratings'])\n    else:\n        continue\n    pred.append(pred_rating)\nprint(len(test_cols))\nprint(len(pred))\n# print(rmse(test_cols, pred))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Following is the method for recommending the books to the user based on their liking as derived from the correlation matrix\n\n#to avoid RuntimeWarning \n#Base class for warnings about dubious runtime behavior.\nimport warnings\nwarnings.filterwarnings(\"ignore\",category = RuntimeWarning)\n\ncorr = np.corrcoef(matrix1)\nprint(corr.shape)\nprint(corr[:10])\n\n#book_ids = books_matrix.columns\ntitle_list = list(books_matrix.columns)\n\n# print(len(title_list))\n# title_list = list(book_ids)\n#print(title_list)\nt = title_list.index(402128)\n\n\ncorr_t = corr[t]\n# print(corr_t)\n# print(corr_t.shape)\na = []\nfor i in range(len(corr_t)):\n    if corr_t[i]>0.95:\n        a.append(title_list[i])\n\nprint(a)\n    ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}